<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta property="og:title" content="Server API" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://clip-as-service.jina.ai/user-guides/server/" />
<meta property="og:site_name" content="CLIP-as-service 0.8.2 Documentation" />
<meta property="og:description" content="CLIP-as-service is designed in a client-server architecture. A server is a long-running program that receives raw sentences and images from clients, and returns CLIP embeddings to the client. Additionally, clip_server is optimized for speed, low memory footprint and scalability. Horizontal scalin..." />
<meta property="og:image" content="https://clip-as-service.jina.ai/_images/server-start.gif" />
<meta property="og:image:alt" content="CLIP-as-service 0.8.2 Documentation" />
<meta name="description" content="CLIP-as-service is designed in a client-server architecture. A server is a long-running program that receives raw sentences and images from clients, and returns CLIP embeddings to the client. Additionally, clip_server is optimized for speed, low memory footprint and scalability. Horizontal scalin..." />
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@JinaAI_">
<meta name="twitter:creator" content="@JinaAI_">
<meta name="description" content="Embed images and sentences into fixed-length vectors via CLIP.">
<meta property="og:description" content="CLIP-as-service is a low-latency high-scalability embedding service for images and texts. It can be easily integrated as a microservice into neural search solutions.">


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E63SXVNDXZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E63SXVNDXZ');
</script>

<script async defer src="https://buttons.github.io/buttons.js"></script>
    
<link rel="index" title="Index" href="../../genindex/" /><link rel="search" title="Search" href="../../search/" /><link rel="next" title="Benchmark" href="../benchmark/" /><link rel="prev" title="Client API" href="../client/" />
        <link rel="canonical" href="https://clip-as-service.jina.ai/user-guides/server.html" />

    <link rel="shortcut icon" href="../../_static/favicon.png"/><!-- Generated with Sphinx 5.3.0 and Furo 2023.03.23 -->
        <title>Server API - CLIP-as-service 0.8.2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    <link rel="stylesheet" type="text/css" href="../../_static/main.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta2/css/all.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: #4d4d4d;
  --color-brand-primary: #009191;
  --color-brand-content: #009191;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
    <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
    <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
    <header class="mobile-header">
        <div class="header-left">
            <label class="nav-overlay-icon" for="__navigation">
                <div class="visually-hidden">Toggle site navigation sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-menu"></use>
                    </svg>
                </i>
            </label>
        </div>
        <div class="header-center">
            <a href="../../">
                <div class="brand">CLIP-as-service 0.8.2 documentation</div>
            </a>
        </div>
        <div class="header-right">
            <div class="theme-toggle-container theme-toggle-header">
                <button class="theme-toggle">
                    <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                    <svg class="theme-icon-when-auto">
                        <use href="#svg-sun-half"></use>
                    </svg>
                    <svg class="theme-icon-when-dark">
                        <use href="#svg-moon"></use>
                    </svg>
                    <svg class="theme-icon-when-light">
                        <use href="#svg-sun"></use>
                    </svg>
                </button>
            </div>
            <label class="toc-overlay-icon toc-header-icon" for="__toc">
                <div class="visually-hidden">Toggle table of contents sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-toc"></use>
                    </svg>
                </i>
            </label>
        </div>
    </header>
    <aside class="sidebar-drawer">
        <div class="sidebar-container">
            
            <div class="sidebar-sticky"><a class="sidebar-brand" href="../../">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../../_static/logo-light.svg" alt="Light Logo" />
    <img class="sidebar-logo only-dark" src="../../_static/logo-dark.svg" alt="Dark Logo" />
  </div>
  
  
</a>
<div class="sd-d-flex-row sd-align-major-spaced">
  <a class="github-button" href="https://github.com/jina-ai/clip-as-service" data-icon="octicon-star" data-show-count="true" aria-label="Star jina-ai/jina on GitHub" style="opacity: 0;">Star</a>
  
</div><form class="sidebar-search-container" method="get" action="../../search/" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
    <p class="caption" role="heading"><span class="caption-text">User Guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../client/">Client API</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Server API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/">Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../retriever/">CLIP Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Hosting</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../hosting/by-jina/">Hosted by Jina AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hosting/on-jcloud/">Host on JCloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hosting/colab/">Host on Google Colab</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../playground/embedding/">Text &amp; Image Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/reasoning/">Visual Reasoning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/searching/">Text &amp; Image Searching</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer References</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api/clip_client/">clip_client package</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api/clip_client.client/">clip_client.client module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/clip_client.helper/">clip_client.helper module</a></li>
</ul>
</li>
</ul>

    <p class="caption" role="heading"><span class="caption-text">Ecosystem</span></p>
    <ul>
        <li class="toctree-l1">
            <a class="reference external" href="https://docs.jina.ai">
                <img class="sidebar-ecosys-logo only-light-line" src="../../_static/search-light.svg">
                <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/search-dark.svg">
                Jina</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://hub.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/hub-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/hub-dark.svg">
            Jina Hub</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://finetuner.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/finetuner-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/finetuner-dark.svg">
            Finetuner</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://docarray.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/docarray-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/docarray-dark.svg">
            DocArray</a></li>
        <li class="toctree-l1"><a class="reference internal" href="#">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/cas-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/cas-dark.svg">
            CLIP-as-service</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://github.com/jina-ai/jcloud">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/JCloud-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/JCloud-dark.svg">
            JCloud</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://now.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/now-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/now-dark.svg">
            NOW</a></li>
    </ul>
</div>
</div>

            </div>
            
        </div>
    </aside>
    <div class="main">
        <div class="content">
            <div class="article-container">
                <a href="#" class="back-to-top muted-link">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
                    </svg>
                    <span>Back to top</span>
                </a>
                <div class="content-icon-container"><div class="theme-toggle-container theme-toggle-content">
                        <button class="theme-toggle">
                            <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                            <svg class="theme-icon-when-auto">
                                <use href="#svg-sun-half"></use>
                            </svg>
                            <svg class="theme-icon-when-dark">
                                <use href="#svg-moon"></use>
                            </svg>
                            <svg class="theme-icon-when-light">
                                <use href="#svg-sun"></use>
                            </svg>
                        </button>
                    </div>
                    <label class="toc-overlay-icon toc-content-icon"
                           for="__toc">
                        <div class="visually-hidden">Toggle table of contents sidebar</div>
                        <i class="icon">
                            <svg>
                                <use href="#svg-toc"></use>
                            </svg>
                        </i>
                    </label>
                </div>
                <article role="main">
                    <section class="tex2jax_ignore mathjax_ignore" id="server-api">
<h1>Server API<a class="headerlink" href="#server-api" title="Permalink to this heading">#</a></h1>
<p>CLIP-as-service is designed in a client-server architecture. A server is a long-running program that receives raw sentences and images from clients, and returns CLIP embeddings to the client. Additionally, <code class="docutils literal notranslate"><span class="pre">clip_server</span></code> is optimized for speed, low memory footprint and scalability.</p>
<ul class="simple">
<li><p>Horizontal scaling: adding more replicas easily with one argument.</p></li>
<li><p>Vertical scaling: using PyTorch JIT, ONNX or TensorRT runtime to speedup single GPU inference.</p></li>
<li><p>Supporting gRPC, HTTP, Websocket protocols with their TLS counterparts, w/o compressions.</p></li>
</ul>
<p>This chapter introduces the API of the server.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You will need to install server first in Python 3.7+: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">clip-server</span></code>.</p>
</div>
<section id="start-server">
<span id="server-address"></span><h2>Start server<a class="headerlink" href="#start-server" title="Permalink to this heading">#</a></h2>
<section id="start-a-pytorch-backed-server">
<h3>Start a PyTorch-backed server<a class="headerlink" href="#start-a-pytorch-backed-server" title="Permalink to this heading">#</a></h3>
<p>Unlike the client, server only has a CLI entrypoint. To start a server, run the following in the terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>clip_server
</pre></div>
</div>
<p>Note that it is underscore <code class="docutils literal notranslate"><span class="pre">_</span></code> not the dash <code class="docutils literal notranslate"><span class="pre">-</span></code>.</p>
<p>First time running will download the pretrained model (Pytorch <code class="docutils literal notranslate"><span class="pre">ViT-B/32</span></code> by default), load the model, and finally you will get the address information of the server. This information will <a class="reference internal" href="../client/#construct-client"><span class="std std-ref">then be used in clients</span></a>.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/server-start.gif"><img alt="../../_images/server-start.gif" src="../../_images/server-start.gif" style="width: 70%;" /></a>
</figure>
</section>
<section id="start-a-onnx-backed-server">
<h3>Start a ONNX-backed server<a class="headerlink" href="#start-a-onnx-backed-server" title="Permalink to this heading">#</a></h3>
<p>To use ONNX runtime for CLIP, you can run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="s2">&quot;clip_server[onnx]&quot;</span>

python<span class="w"> </span>-m<span class="w"> </span>clip_server<span class="w"> </span>onnx-flow.yml
</pre></div>
</div>
</section>
<section id="start-a-tensorrt-backed-server">
<h3>Start a TensorRT-backed server<a class="headerlink" href="#start-a-tensorrt-backed-server" title="Permalink to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">nvidia-pyindex</span></code> package needs to be installed first. It allows your <code class="docutils literal notranslate"><span class="pre">pip</span></code> to fetch additional Python modules from the NVIDIA NGC‚Ñ¢ PyPI repo:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>nvidia-pyindex
pip<span class="w"> </span>install<span class="w"> </span><span class="s2">&quot;clip_server[tensorrt]&quot;</span>

python<span class="w"> </span>-m<span class="w"> </span>clip_server<span class="w"> </span>tensorrt-flow.yml
</pre></div>
</div>
<p>One may wonder where is this <code class="docutils literal notranslate"><span class="pre">onnx-flow.yml</span></code> or <code class="docutils literal notranslate"><span class="pre">tensorrt-flow.yml</span></code> come from. Must be a typo? Believe me, just run it. It should just work. I will explain this YAML file in the next section.</p>
<p>The procedure and UI of ONNX and TensorRT runtime would look the same as Pytorch runtime.</p>
</section>
</section>
<section id="model-support">
<h2>Model support<a class="headerlink" href="#model-support" title="Permalink to this heading">#</a></h2>
<p>The various <code class="docutils literal notranslate"><span class="pre">CLIP</span></code> models implemented in the <a class="reference external" href="https://github.com/openai/CLIP">OpenAI</a>, <a class="reference external" href="https://github.com/mlfoundations/open_clip">OpenCLIP</a>, and <a class="reference external" href="https://github.com/FreddeFrallan/Multilingual-CLIP">MultilingualCLIP</a> are supported.
<code class="docutils literal notranslate"><span class="pre">ViT-B-32::openai</span></code> is used as the default model in all runtimes.
Due to the limitation of some runtimes, not every runtime supports all models.
Please also note that <strong>different models give different sizes of output dimensions</strong>. This will affect your downstream applications. For example, switching the model from one to another make your embedding incomparable, which breaks the downstream applications. Below is a list of supported models of each runtime and its corresponding size.</p>
<p>For more details about the models and how to select the best model for your application, please refer to the <a class="reference internal" href="../benchmark/"><span class="doc std std-doc">CLIP benchmark page</span></a>.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>ONNX</p></th>
<th class="head"><p>TensorRT</p></th>
<th class="head"><p>Output Dimension</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>RN50::openai</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-odd"><td><p>RN50::yfcc15m</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-even"><td><p>RN50::cc12m</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-odd"><td><p>RN101::openai</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-even"><td><p>RN101::yfcc15m</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-odd"><td><p>RN50x4::openai</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>640</p></td>
</tr>
<tr class="row-even"><td><p>RN50x16::openai</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>768</p></td>
</tr>
<tr class="row-odd"><td><p>RN50x64::openai</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::openai</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::laion2b_e16</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::laion400m_e31</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::laion400m_e32</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::laion2b-s34b-b79k</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16::openai</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16::laion400m_e31</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16::laion400m_e32</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16-plus-240::laion400m_e31</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>üöß</p></td>
<td><p>640</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16-plus-240::laion400m_e32</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>üöß</p></td>
<td><p>640</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14::openai</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>768</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14::laion400m_e31</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>768</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14::laion400m_e32</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>768</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14::laion2b-s32b-b82k</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>768</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14-336::openai</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>768</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-H-14::laion2b-s32b-b79k</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-even"><td><p>ViT-g-14::laion2b-s12b-b42k</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-odd"><td><p>M-CLIP/LABSE-Vit-L-14</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>768</p></td>
</tr>
<tr class="row-even"><td><p>M-CLIP/XLM-Roberta-Large-Vit-B-32</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>üöß</p></td>
<td><p>512</p></td>
</tr>
<tr class="row-odd"><td><p>M-CLIP/XLM-Roberta-Large-Vit-B-16Plus</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>üöß</p></td>
<td><p>640</p></td>
</tr>
<tr class="row-even"><td><p>M-CLIP/XLM-Roberta-Large-Vit-L-14</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚úÖ</p></td>
<td><p>‚ùå</p></td>
<td><p>768</p></td>
</tr>
</tbody>
</table>
</div>
<p>‚úÖ = Supported ‚Äî üöß = Working in progress ‚Äî ‚ùå = Not supported</p>
<section id="use-custom-model-for-onnx">
<h3>Use custom model for onnx<a class="headerlink" href="#use-custom-model-for-onnx" title="Permalink to this heading">#</a></h3>
<p>You can also use your own model in ONNX runtime by specifying the model name and the path to ONNX model directory in YAML file.
The model directory should have the same structure as below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>.
‚îî‚îÄ‚îÄ custom-model/
    ‚îú‚îÄ‚îÄ textual.onnx
    ‚îî‚îÄ‚îÄ visual.onnx
</pre></div>
</div>
<p>One may wonder how to produce the model as described above.
Fortunately, you can simply use the <a class="reference external" href="https://finetuner.jina.ai">Finetuner</a> to fine-tune your model based on custom dataset.
<a class="reference external" href="https://finetuner.jina.ai">Finetuner</a> is a cloud service that makes fine-tuning simple and fast.
Moving the process into the cloud, <a class="reference external" href="https://finetuner.jina.ai">Finetuner</a> handles all related complexity and infrastructure, making models performant and production ready.
<a class="reference internal" href="../finetuner/#finetuner"><span class="std std-ref">Click here for detail instructions</span></a>.</p>
</section>
</section>
<section id="yaml-config">
<h2>YAML config<a class="headerlink" href="#yaml-config" title="Permalink to this heading">#</a></h2>
<p>You may notice that there is a YAML file in our last ONNX example. All configurations are stored in this file. In fact, <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">clip_server</span></code> does <strong>not support</strong> any other argument besides a YAML file. So it is the only source of the truth of your configs.</p>
<p>To load a YAML config from <code class="docutils literal notranslate"><span class="pre">my.yml</span></code>, simply do</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>clip_server<span class="w"> </span>my.yml
</pre></div>
</div>
<p>Or one can also pipe the config via stdin:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>my.yml<span class="w"> </span><span class="p">|</span><span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>clip_server<span class="w"> </span>-i
</pre></div>
</div>
<p>This can be very useful when using <code class="docutils literal notranslate"><span class="pre">clip_server</span></code> in a Docker container.</p>
<p>And to answer your doubt, <code class="docutils literal notranslate"><span class="pre">clip_server</span></code> has three built-in YAML configs as a part of the package resources. When you do <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">clip_server</span></code> it loads the Pytorch config, and when you do <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">clip_server</span> <span class="pre">onnx-flow.yml</span></code> it loads the ONNX config.
In the same way, when you do <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">clip_server</span> <span class="pre">tensorrt-flow.yml</span></code> it loads the TensorRT config.</p>
<p>Let‚Äôs look at these three built-in YAML configs:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--1">torch-flow.yml</label><div class="tab-content docutils">
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_t</span>
<span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_torch</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--2">onnx-flow.yml</label><div class="tab-content docutils">
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_o</span>
<span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_onnx</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--0-input--3" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--3">tensorrt-flow.yml</label><div class="tab-content docutils">
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_r</span>
<span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_tensorrt</span>
</pre></div>
</div>
</div>
</div>
<p>Basically, each YAML file defines a <a class="reference external" href="https://docs.jina.ai/fundamentals/flow/">Jina Flow</a>. The complete Jina Flow YAML syntax <a class="reference external" href="https://docs.jina.ai/fundamentals/flow/yaml-spec/">can be found here</a>. General parameters of the Flow and Executor can be used here as well. But now we only highlight the most important parameters.</p>
<p>Looking at the YAML file again, we can put it into three subsections as below:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--1-input--1" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--1">CLIP model config</label><div class="tab-content docutils">
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_t</span>
<span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="hll"><span class="w">      </span><span class="nt">with</span><span class="p">:</span>
</span><span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_torch</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--1-input--2" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--2">Executor config</label><div class="tab-content docutils">
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="nt">executors</span><span class="p">:</span>
<span class="hll"><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_t</span>
</span><span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="w">      </span><span class="nt">with</span><span class="p">:</span><span class="w"> </span>
<span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_torch</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--1-input--3" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--3">Flow config</label><div class="tab-content docutils">
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="hll"><span class="nt">with</span><span class="p">:</span>
</span><span class="hll"><span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
</span><span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_t</span>
<span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="w">      </span><span class="nt">with</span><span class="p">:</span><span class="w"> </span>
<span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_torch</span>
</pre></div>
</div>
</div>
</div>
<section id="clip-model-config">
<h3>CLIP model config<a class="headerlink" href="#clip-model-config" title="Permalink to this heading">#</a></h3>
<p>For all backends, you can set the following parameters via <code class="docutils literal notranslate"><span class="pre">with</span></code>:</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">name</span></code></p></td>
<td><p>The name of the model to be used. Default ‚ÄòViT-B-32::openai‚Äô. A list of available models can be found <a class="reference external" href="#model-support">here</a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">num_worker_preprocess</span></code></p></td>
<td><p>The number of CPU workers to preprocess images and texts. Default is 4.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">minibatch_size</span></code></p></td>
<td><p>The size of the minibatch for preprocessing and encoding. Default is 32. Reduce this number if you encounter OOM errors.</p></td>
</tr>
</tbody>
</table>
</div>
<p>There are also runtime-specific parameters listed below:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--2-input--1" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--1">PyTorch</label><div class="tab-content docutils">
<div class="table-wrapper colwidths-auto docutils container">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">device</span></code></p></td>
<td><p>‚Äòcpu‚Äô or ‚Äòcuda‚Äô. Default is None, which auto-detects the device.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">jit</span></code></p></td>
<td><p>Whether to use JIT compilation. Default is False.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<input class="tab-input" id="tab-set--2-input--2" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--2">ONNX</label><div class="tab-content docutils">
<div class="table-wrapper colwidths-auto docutils container">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">device</span></code></p></td>
<td><p>‚Äòcpu‚Äô or ‚Äòcuda‚Äô. Default is None, which auto-detects the device.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">model_path</span></code></p></td>
<td><p>The path to the model to be used. If not specified, the model will be downloaded or loaded from the local cache. See <a class="reference external" href="#use-custom-model-for-onnx">here</a> to learn how to finetune custom models.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>For example, to turn on JIT and force PyTorch running on CPU, one can do:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_t</span>
<span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="hll"><span class="w">      </span><span class="nt">with</span><span class="p">:</span><span class="w"> </span>
</span><span class="hll"><span class="w">        </span><span class="nt">jit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
</span><span class="hll"><span class="w">        </span><span class="nt">device</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cpu</span>
</span><span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_torch</span>
</pre></div>
</div>
<p>To use custom model in ONNX runtime, one can do:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_o</span>
<span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="hll"><span class="w">      </span><span class="nt">with</span><span class="p">:</span>
</span><span class="hll"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ViT-B/32</span>
</span><span class="hll"><span class="w">        </span><span class="nt">model_path</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;custom-model&#39;</span>
</span><span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_onnx</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The model name should match the fine-tuned model, or you will get incorrect output.</p>
</div>
</section>
<section id="executor-config">
<h3>Executor config<a class="headerlink" href="#executor-config" title="Permalink to this heading">#</a></h3>
<p>The full list of configs for Executor can be found via <code class="docutils literal notranslate"><span class="pre">jina</span> <span class="pre">executor</span> <span class="pre">--help</span></code>. The most important one is probably <code class="docutils literal notranslate"><span class="pre">replicas</span></code>, which <strong>allows you to run multiple CLIP models in parallel</strong> to achieve horizontal scaling.</p>
<p>To scale to 4 CLIP replicas, simply adding <code class="docutils literal notranslate"><span class="pre">replicas:</span> <span class="pre">4</span></code> under <code class="docutils literal notranslate"><span class="pre">uses:</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_t</span>
<span class="hll"><span class="w">    </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</span><span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_torch</span>
</pre></div>
</div>
</section>
<section id="flow-config">
<span id="id1"></span><h3>Flow config<a class="headerlink" href="#flow-config" title="Permalink to this heading">#</a></h3>
<p>Flow configs are the ones under top-level <code class="docutils literal notranslate"><span class="pre">with:</span></code>. We can see the <code class="docutils literal notranslate"><span class="pre">port:</span> <span class="pre">51000</span></code> is configured there. Besides <code class="docutils literal notranslate"><span class="pre">port</span></code>, there are some common parameters you might need.</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">protocol</span></code></p></td>
<td><p>Communication protocol between server and client.  Can be <code class="docutils literal notranslate"><span class="pre">grpc</span></code>, <code class="docutils literal notranslate"><span class="pre">http</span></code>, <code class="docutils literal notranslate"><span class="pre">websocket</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cors</span></code></p></td>
<td><p>Only effective when <code class="docutils literal notranslate"><span class="pre">protocol=http</span></code>. If set, a CORS middleware is added to FastAPI frontend to allow cross-origin access.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">prefetch</span></code></p></td>
<td><p>Control the maximum streamed request inside the Flow at any given time, default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, means no limit. Setting <code class="docutils literal notranslate"><span class="pre">prefetch</span></code> to a small number helps solving the OOM problem, but may slow down the streaming a bit.</p></td>
</tr>
</tbody>
</table>
</div>
<p>As an example, to set <code class="docutils literal notranslate"><span class="pre">protocol</span></code> and <code class="docutils literal notranslate"><span class="pre">prefetch</span></code>, one can modify the YAML as follows:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="hll"><span class="w">  </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">websocket</span>
</span><span class="hll"><span class="w">  </span><span class="nt">prefetch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
</span><span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_t</span>
<span class="w">    </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_torch</span>
</pre></div>
</div>
</section>
</section>
<section id="environment-variables">
<h2>Environment variables<a class="headerlink" href="#environment-variables" title="Permalink to this heading">#</a></h2>
<p>To start a server with more verbose logging,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">JINA_LOG_LEVEL</span><span class="o">=</span>DEBUG<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>clip_server
</pre></div>
</div>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/server-log.gif"><img alt="../../_images/server-log.gif" src="../../_images/server-log.gif" style="width: 70%;" /></a>
</figure>
<p>To run CLIP-server on 3rd GPU,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">2</span><span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>clip_server
</pre></div>
</div>
<section id="serve-on-multiple-gpus">
<h3>Serve on Multiple GPUs<a class="headerlink" href="#serve-on-multiple-gpus" title="Permalink to this heading">#</a></h3>
<p>If you have multiple GPU devices, you can leverage them via <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES=RR</span></code>. For example, if you have 3 GPUs and your Flow YAML says <code class="docutils literal notranslate"><span class="pre">replicas:</span> <span class="pre">5</span></code>, then</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>RR<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>clip_server
</pre></div>
</div>
<p>Will assign GPU devices to the following round-robin fashion:</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>GPU device</p></th>
<th class="head"><p>Replica ID</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>4</p></td>
</tr>
</tbody>
</table>
</div>
<p>You can also restrict the visible devices in round-robin assigment by <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES=RR0:2</span></code>, where <code class="docutils literal notranslate"><span class="pre">0:2</span></code> has the same meaning as Python slice. This will create the following assigment:</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>GPU device</p></th>
<th class="head"><p>Replica ID</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>4</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In pratice, we found it is unnecessary to run <code class="docutils literal notranslate"><span class="pre">clip_server</span></code> on multiple GPUs for two reasons:</p>
<ul class="simple">
<li><p>A single replica even with largest <code class="docutils literal notranslate"><span class="pre">ViT-L/14-336px</span></code> takes only 3.5GB VRAM.</p></li>
<li><p>Real network traffic never utilizes GPU in 100%.</p></li>
</ul>
<p>Based on these two points, it makes more sense to have multiple replicas on a single GPU comparing to have multiple replicas on different GPU, which is kind of waste of resources. <code class="docutils literal notranslate"><span class="pre">clip_server</span></code> scales pretty well by interleaving the GPU time with mulitple replicas.</p>
</div>
</section>
</section>
<section id="monitor-with-prometheus-and-grafana">
<h2>Monitor with Prometheus and Grafana<a class="headerlink" href="#monitor-with-prometheus-and-grafana" title="Permalink to this heading">#</a></h2>
<p>To monitor the performance of the service, you can enable the Prometheus metrics in the Flow YAML:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">51000</span>
<span class="hll"><span class="w">  </span><span class="nt">monitoring</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
</span><span class="hll"><span class="w">  </span><span class="nt">port_monitoring</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9090</span>
</span><span class="nt">executors</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_t</span>
<span class="w">    </span><span class="nt">uses</span><span class="p">:</span>
<span class="w">      </span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CLIPEncoder</span>
<span class="w">      </span><span class="nt">metas</span><span class="p">:</span>
<span class="w">        </span><span class="nt">py_modules</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">clip_server.executors.clip_torch</span>
<span class="hll"><span class="w">    </span><span class="nt">monitoring</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span class="hll"><span class="w">    </span><span class="nt">port_monitoring</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9091</span>
</span></pre></div>
</div>
<p>This enables Prometheus metrics on both Gateway and the CLIP Executor.</p>
<p>Running it gives you:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/server-start-monitoring.gif"><img alt="../../_images/server-start-monitoring.gif" src="../../_images/server-start-monitoring.gif" style="width: 80%;" /></a>
</figure>
<p>which exposes two additional endpoints:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">http://localhost:9090</span></code>  for the Gateway</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">http://localhost:9091</span></code>  for the CLIP Executor</p></li>
</ul>
<p>To visualize the metrics in Grafana, you can import this <a class="reference external" href="https://clip-as-service.jina.ai/_static/cas-grafana.json">JSON file of an example dashboard</a>. You will get something as follows:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/grafana-dashboard.png"><img alt="../../_images/grafana-dashboard.png" src="../../_images/grafana-dashboard.png" style="width: 80%;" /></a>
</figure>
<p>For more information on monitoring a Flow, <a class="reference external" href="https://docs.jina.ai/fundamentals/flow/monitoring-flow/">please read here</a>.</p>
</section>
<section id="serve-with-tls">
<h2>Serve with TLS<a class="headerlink" href="#serve-with-tls" title="Permalink to this heading">#</a></h2>
<p>You can turn on TLS for HTTP and gRPC protocols. Your Flow YAML should be changed to the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flow</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;1&#39;</span>
<span class="nt">with</span><span class="p">:</span>
<span class="hll"><span class="w">  </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8443</span>
</span><span class="hll"><span class="w">  </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http</span>
</span><span class="w">  </span><span class="nt">cors</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="hll"><span class="w">  </span><span class="nt">uvicorn_kwargs</span><span class="p">:</span>
</span><span class="hll"><span class="w">    </span><span class="nt">ssl_keyfile_password</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">blahblah</span>
</span><span class="hll"><span class="w">  </span><span class="nt">ssl_certfile</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cert.pem</span>
</span><span class="hll"><span class="w">  </span><span class="nt">ssl_keyfile</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">key.pem</span>
</span></pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">protocol</span></code> can be either <code class="docutils literal notranslate"><span class="pre">http</span></code> or <code class="docutils literal notranslate"><span class="pre">grpc</span></code>; <code class="docutils literal notranslate"><span class="pre">cert.pem</span></code> or <code class="docutils literal notranslate"><span class="pre">key.pem</span></code> represent both parts of a certificate, key being the private key to the certificate and crt being the signed certificate. You can run the following command in terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>openssl<span class="w"> </span>req<span class="w"> </span>-newkey<span class="w"> </span>rsa:4096<span class="w"> </span>-nodes<span class="w"> </span>-sha512<span class="w"> </span>-x509<span class="w"> </span>-days<span class="w"> </span><span class="m">3650</span><span class="w"> </span>-nodes<span class="w"> </span>-out<span class="w"> </span>cert.pem<span class="w"> </span>-keyout<span class="w"> </span>key.pem<span class="w"> </span>-subj<span class="w"> </span><span class="s2">&quot;/CN=&lt;your.clip.address&gt;&quot;</span>
</pre></div>
</div>
<p>Note that if you are using <code class="docutils literal notranslate"><span class="pre">protocol:</span> <span class="pre">grpc</span></code> then <code class="docutils literal notranslate"><span class="pre">/CN=&lt;your.clip.address&gt;</span></code> must strictly follow the IP address or the domain name of your server. Mismatch IP or domain name would throw an exception.</p>
<p>Certificate and keys can be also generated via <a class="reference external" href="https://letsencrypt.org/">letsencrypt.org</a>, which is a free SSL provider.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that note every port support HTTPS. Commonly support ports are: <code class="docutils literal notranslate"><span class="pre">443</span></code>, <code class="docutils literal notranslate"><span class="pre">2053</span></code>, <code class="docutils literal notranslate"><span class="pre">2083</span></code>, <code class="docutils literal notranslate"><span class="pre">2087</span></code>, <code class="docutils literal notranslate"><span class="pre">2096</span></code>, <code class="docutils literal notranslate"><span class="pre">8443</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you are using Cloudflare proxied DNS, please be aware:</p>
<ul class="simple">
<li><p>you need to turn on gRPC support manually, <a class="reference external" href="https://support.cloudflare.com/hc/en-us/articles/360050483011-Understanding-Cloudflare-gRPC-support">please follow the guide here</a>;</p></li>
<li><p>the free tier of Cloudflare has 100s hard limit on the timeout, meaning sending big batch to a CPU server may throw 524 to the client-side.</p></li>
</ul>
</div>
<p>When the server is successfully running, you can connect to it via client by setting <code class="docutils literal notranslate"><span class="pre">server</span></code> to <code class="docutils literal notranslate"><span class="pre">https://</span></code> or <code class="docutils literal notranslate"><span class="pre">grpcs://</span></code> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">clip_client</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s1">&#39;grpcs://&lt;your.clip.address&gt;:2096&#39;</span><span class="p">)</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="s1">&#39;First do it&#39;</span><span class="p">,</span>
        <span class="s1">&#39;then do it right&#39;</span><span class="p">,</span>
        <span class="s1">&#39;then do it better&#39;</span><span class="p">,</span>
        <span class="s1">&#39;https://picsum.photos/200&#39;</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="serve-in-docker-container">
<h2>Serve in Docker Container<a class="headerlink" href="#serve-in-docker-container" title="Permalink to this heading">#</a></h2>
<p>You can run the server inside a Docker container. We provide a Dockerfile in the repository, which is CUDA-enabled with optimized package installation.</p>
<section id="build">
<h3>Build<a class="headerlink" href="#build" title="Permalink to this heading">#</a></h3>
<p>We have a list of <a class="reference internal" href="#prebuild-images"><span class="std std-ref">pre-built images available on Docker Hub</span></a>. If they are too big for you to download, you may consider built it yourself as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/jina-ai/clip-as-service.git
docker<span class="w"> </span>build<span class="w"> </span>.<span class="w"> </span>-f<span class="w"> </span>Dockerfiles/server.Dockerfile<span class="w">  </span>--build-arg<span class="w"> </span><span class="nv">GROUP_ID</span><span class="o">=</span><span class="k">$(</span>id<span class="w"> </span>-g<span class="w"> </span><span class="si">${</span><span class="nv">USER</span><span class="si">}</span><span class="k">)</span><span class="w"> </span>--build-arg<span class="w"> </span><span class="nv">USER_ID</span><span class="o">=</span><span class="k">$(</span>id<span class="w"> </span>-u<span class="w"> </span><span class="si">${</span><span class="nv">USER</span><span class="si">}</span><span class="k">)</span><span class="w"> </span>-t<span class="w"> </span>jinaai/clip-server
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The build argument <code class="docutils literal notranslate"><span class="pre">--build-arg</span> <span class="pre">GROUP_ID=$(id</span> <span class="pre">-g</span> <span class="pre">${USER})</span> <span class="pre">--build-arg</span> <span class="pre">USER_ID=$(id</span> <span class="pre">-u</span> <span class="pre">${USER})</span></code> is optional, but having them is highly recommended as it allows you to reuse host‚Äôs cache with the correct access.</p>
</div>
</section>
<section id="run">
<h3>Run<a class="headerlink" href="#run" title="Permalink to this heading">#</a></h3>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--3-input--1" name="tab-set--3" type="radio"><label class="tab-label" for="tab-set--3-input--1">PyTorch</label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-p<span class="w"> </span><span class="m">51009</span>:51000<span class="w"> </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache:/home/cas/.cache<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>jinaai/clip-server
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--3-input--2" name="tab-set--3" type="radio"><label class="tab-label" for="tab-set--3-input--2">ONNX</label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-p<span class="w"> </span><span class="m">51009</span>:51000<span class="w"> </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache:/home/cas/.cache<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>jinaai/clip-server:master-onnx<span class="w"> </span>onnx-flow.yml
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--3-input--3" name="tab-set--3" type="radio"><label class="tab-label" for="tab-set--3-input--3">TensorRT</label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-p<span class="w"> </span><span class="m">51009</span>:51000<span class="w"> </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache:/home/cas/.cache<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>jinaai/clip-server:master-tensorrt<span class="w"> </span>tensorrt-flow.yml
</pre></div>
</div>
</div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">51009</span></code> is the public port on the host and <code class="docutils literal notranslate"><span class="pre">51000</span></code> is the <a class="reference internal" href="#flow-config"><span class="std std-ref">in-container port defined inside YAML</span></a>. The argument <code class="docutils literal notranslate"><span class="pre">-v</span> <span class="pre">$HOME/.cache:/home/cas/.cache</span></code> leverages host‚Äôs cache and prevents you to download the same model next time on start.</p>
<p>Due to the limitation of the terminal inside Docker container, you will <strong>not</strong> see the classic Jina progress bar on start. Instead, you will face a few minutes awkward silent while model downloading and then see ‚ÄúFlow is ready to serve‚Äù dialog.</p>
<p>To pass a YAML config from the host, one can do:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--4-input--1" name="tab-set--4" type="radio"><label class="tab-label" for="tab-set--4-input--1">PyTorch</label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>my.yml<span class="w"> </span><span class="p">|</span><span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>-i<span class="w"> </span>-p<span class="w"> </span><span class="m">51009</span>:51000<span class="w"> </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache:/home/cas/.cache<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>jinaai/clip-server<span class="w"> </span>-i
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--4-input--2" name="tab-set--4" type="radio"><label class="tab-label" for="tab-set--4-input--2">ONNX</label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>my.yml<span class="w"> </span><span class="p">|</span><span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>-i<span class="w"> </span>-p<span class="w"> </span><span class="m">51009</span>:51000<span class="w"> </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache:/home/cas/.cache<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>jinaai/clip-server:master-onnx<span class="w"> </span>-i
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--4-input--3" name="tab-set--4" type="radio"><label class="tab-label" for="tab-set--4-input--3">TensorRT</label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>my.yml<span class="w"> </span><span class="p">|</span><span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>-i<span class="w"> </span>-p<span class="w"> </span><span class="m">51009</span>:51000<span class="w"> </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache:/home/cas/.cache<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>jinaai/clip-server:master-tensorrt<span class="w"> </span>-i
</pre></div>
</div>
</div>
</div>
<p>The CLI usage is the same <a class="reference internal" href="#server-address"><span class="std std-ref">as described here</span></a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can enable debug logging via: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">--env</span> <span class="pre">JINA_LOG_LEVEL=debug</span> <span class="pre">...</span></code></p>
</div>
</section>
<section id="pre-built-images">
<span id="prebuild-images"></span><h3>Pre-built images<a class="headerlink" href="#pre-built-images" title="Permalink to this heading">#</a></h3>
<p>We have prebuilt images with CUDA support.</p>
<p>The Docker image name always starts with <code class="docutils literal notranslate"><span class="pre">jinaai/clip-server</span></code> followed by a tag composed of three parts:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>jinaai/clip-server:{version}{extra}
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">{version}</span></code>: The version of Jina. Possible values:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">latest</span></code>: the last release;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">master</span></code>: the master branch of <code class="docutils literal notranslate"><span class="pre">jina-ai/jina</span></code> repository;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x.y.z</span></code>: the release of a particular version;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x.y</span></code> and <code class="docutils literal notranslate"><span class="pre">x</span></code>: the alias to the last <code class="docutils literal notranslate"><span class="pre">x.y.z</span></code> patch release, i.e. <code class="docutils literal notranslate"><span class="pre">x.y</span></code> = <code class="docutils literal notranslate"><span class="pre">x.y.max(z)</span></code>;</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">{extra}</span></code>: the extra dependency installed along with <code class="docutils literal notranslate"><span class="pre">clip_server</span></code>. Possible values:</p>
<ul>
<li><p><code class="docutils literal notranslate"> </code>: Pytorch backend;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-onnx</span></code>: ONNX backend;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-tensorrt</span></code>: TensorRT backend;</p></li>
</ul>
</li>
</ul>
<section id="image-alias-and-updates">
<h4>Image alias and updates<a class="headerlink" href="#image-alias-and-updates" title="Permalink to this heading">#</a></h4>
<div class="table-wrapper colwidths-auto docutils container">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Event</p></th>
<th class="head"><p>Updated images</p></th>
<th class="head"><p>Aliases</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>On merge into <code class="docutils literal notranslate"><span class="pre">main</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">jinaai/clip-server:master{extra}</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>On <code class="docutils literal notranslate"><span class="pre">x.y.z</span></code> release</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">jinaai/clip-server:x.y.z{extra}</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">jinaai/clip-server:latest{python_version}{extra}</span></code>, <code class="docutils literal notranslate"><span class="pre">jinaai/clip-server:x.y{python_version}{extra}</span></code>, <code class="docutils literal notranslate"><span class="pre">jinaai/clip-server:x{python_version}{extra}</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p>3 images are built on the event listed above, i.e. taking the combination of:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">{extra}</span> <span class="pre">=</span> <span class="pre">[&quot;&quot;,</span> <span class="pre">&quot;-onnx&quot;,</span> <span class="pre">&quot;-tensorrt&quot;]</span></code></p></li>
</ul>
</section>
<section id="image-size-on-different-tags">
<h4>Image size on different tags<a class="headerlink" href="#image-size-on-different-tags" title="Permalink to this heading">#</a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference external" href="https://github.com/badges/shields/issues/7583">Due to a known bug in shields.io/Docker Hub API</a>, the following badge may show ‚Äúinvalid‚Äù status randomly.</p>
</div>
<div class="table-wrapper colwidths-auto docutils container">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Image Size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img alt="" src="https://img.shields.io/docker/image-size/jinaai/clip-server/latest?label=jinaai%2Fclip-server%3Alatest&amp;logo=docker" /></p></td>
</tr>
<tr class="row-odd"><td><p><img alt="" src="https://img.shields.io/docker/image-size/jinaai/clip-server/latest-onnx?label=jinaai%2Fclip-server%3Alatest-onnx&amp;logo=docker" /></p></td>
</tr>
<tr class="row-even"><td><p><img alt="" src="https://img.shields.io/docker/image-size/jinaai/clip-server/latest-tensorrt?label=jinaai%2Fclip-server%3Alatest-tensorrt&amp;logo=docker" /></p></td>
</tr>
<tr class="row-odd"><td><p><img alt="" src="https://img.shields.io/docker/image-size/jinaai/clip-server/master?label=jinaai%2Fclip-server%3Amaster&amp;logo=docker" /></p></td>
</tr>
<tr class="row-even"><td><p><img alt="" src="https://img.shields.io/docker/image-size/jinaai/clip-server/master-onnx?label=jinaai%2Fclip-server%3Amaster-onnx&amp;logo=docker" /></p></td>
</tr>
<tr class="row-odd"><td><p><img alt="" src="https://img.shields.io/docker/image-size/jinaai/clip-server/master-tensorrt?label=jinaai%2Fclip-server%3Amaster-tensorrt&amp;logo=docker" /></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
</section>

                </article>
            </div>
            <footer>
                
                <div class="related-pages">
                    <a class="next-page" href="../benchmark/">
                        <div class="page-info">
                            <div class="context">
                                <span>Next</span>
                            </div>
                            <div class="title">Benchmark</div>
                        </div>
                        <svg class="furo-related-icon">
                            <use href="#svg-arrow-right"></use>
                        </svg>
                    </a>
                    <a class="prev-page" href="../client/">
                        <svg class="furo-related-icon">
                            <use href="#svg-arrow-right"></use>
                        </svg>
                        <div class="page-info">
                            <div class="context">
                                <span>Previous</span>
                            </div>
                            
                            <div class="title">Client API</div>
                            
                        </div>
                    </a>
                </div>
                <div class="bottom-of-page">
                    <div class="left-details">
                        <div class="copyright">
                            Copyright &#169; Jina AI Limited. All rights reserved.
                        </div><div class="last-updated">
                            Last updated on Mar 26, 2023</div>
                    </div>
                    <div class="right-details">
                        <div class="social-btns">
                            <a class='social-btn' href="https://github.com/jina-ai/clip-as-service/" aria-label="GitHub"
                               target="_blank" rel="noreferrer"> <i class="fab fa-github"></i></a>
                            <a class='social-btn' href="https://slack.jina.ai" aria-label="Slack" target="_blank"
                               rel="noreferrer"> <i class="fab fa-slack"></i></a>
                            <a class='social-btn' href="https://youtube.com/c/jina-ai" aria-label="YouTube"
                               target="_blank" rel="noreferrer"> <i class="fab fa-youtube"></i></a>
                            <a class='social-btn' href="https://twitter.com/JinaAI_" aria-label="Twitter"
                               target="_blank" rel="noreferrer"> <i class="fab fa-twitter"></i></a>
                            <a class='social-btn' href="https://www.linkedin.com/company/jinaai/" aria-label="LinkedIn"
                               target="_blank" rel="noreferrer"> <i class="fab fa-linkedin"></i></a>
                        </div>
                    </div>
                </div>
                
            </footer>
        </div>
        <aside class="toc-drawer">
            
            
            <div class="toc-sticky toc-scroll">
                <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
                </div>
                <div class="toc-tree-container">
                    <div class="toc-tree">
                        <ul>
<li><a class="reference internal" href="#">Server API</a><ul>
<li><a class="reference internal" href="#start-server">Start server</a><ul>
<li><a class="reference internal" href="#start-a-pytorch-backed-server">Start a PyTorch-backed server</a></li>
<li><a class="reference internal" href="#start-a-onnx-backed-server">Start a ONNX-backed server</a></li>
<li><a class="reference internal" href="#start-a-tensorrt-backed-server">Start a TensorRT-backed server</a></li>
</ul>
</li>
<li><a class="reference internal" href="#model-support">Model support</a><ul>
<li><a class="reference internal" href="#use-custom-model-for-onnx">Use custom model for onnx</a></li>
</ul>
</li>
<li><a class="reference internal" href="#yaml-config">YAML config</a><ul>
<li><a class="reference internal" href="#clip-model-config">CLIP model config</a></li>
<li><a class="reference internal" href="#executor-config">Executor config</a></li>
<li><a class="reference internal" href="#flow-config">Flow config</a></li>
</ul>
</li>
<li><a class="reference internal" href="#environment-variables">Environment variables</a><ul>
<li><a class="reference internal" href="#serve-on-multiple-gpus">Serve on Multiple GPUs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#monitor-with-prometheus-and-grafana">Monitor with Prometheus and Grafana</a></li>
<li><a class="reference internal" href="#serve-with-tls">Serve with TLS</a></li>
<li><a class="reference internal" href="#serve-in-docker-container">Serve in Docker Container</a><ul>
<li><a class="reference internal" href="#build">Build</a></li>
<li><a class="reference internal" href="#run">Run</a></li>
<li><a class="reference internal" href="#pre-built-images">Pre-built images</a><ul>
<li><a class="reference internal" href="#image-alias-and-updates">Image alias and updates</a></li>
<li><a class="reference internal" href="#image-size-on-different-tags">Image size on different tags</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

                    </div>
                </div>
            </div>
            
            
        </aside>
    </div>
</div>
<img referrerpolicy="no-referrer-when-downgrade"
     src="https://static.scarf.sh/a.png?x-pxid=2823e771-0e1e-4320-8fde-48bc48e53262"/><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/vue@2/dist/vue.min.js"></script>
    </body>
</html>