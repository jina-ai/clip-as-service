<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta property="og:title" content="Benchmark" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://clip-as-service.jina.ai/user-guides/benchmark/" />
<meta property="og:site_name" content="CLIP-as-service 0.8.1 Documentation" />
<meta property="og:description" content="In order to understand the zero-shot performance of CLIP and its limitations, we conducted a benchmark across a variety of computer vision datasets (the dataset details are in the appendix). Here, thanks for the open-source CLIP Benchmark toolkit, we can easily reproduce the results. We hope that..." />
<meta property="og:image" content="https://clip-as-service.jina.ai/_static/banner.png" />
<meta property="og:image:alt" content="CLIP-as-service 0.8.1 Documentation" />
<meta name="description" content="In order to understand the zero-shot performance of CLIP and its limitations, we conducted a benchmark across a variety of computer vision datasets (the dataset details are in the appendix). Here, thanks for the open-source CLIP Benchmark toolkit, we can easily reproduce the results. We hope that..." />
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@JinaAI_">
<meta name="twitter:creator" content="@JinaAI_">
<meta name="description" content="Embed images and sentences into fixed-length vectors via CLIP.">
<meta property="og:description" content="CLIP-as-service is a low-latency high-scalability embedding service for images and texts. It can be easily integrated as a microservice into neural search solutions.">


<script async src="https://www.googletagmanager.com/gtag/js?id=G-E63SXVNDXZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E63SXVNDXZ');
</script>

<script async defer src="https://buttons.github.io/buttons.js"></script>
    
<link rel="index" title="Index" href="../../genindex/" /><link rel="search" title="Search" href="../../search/" /><link rel="next" title="CLIP Search" href="../retriever/" /><link rel="prev" title="Server API" href="../server/" />
        <link rel="canonical" href="https://clip-as-service.jina.ai/user-guides/benchmark.html" />

    <link rel="shortcut icon" href="../../_static/favicon.png"/><meta name="generator" content="sphinx-4.5.0, furo 2022.09.29"/>
        <title>Benchmark - CLIP-as-service 0.8.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=d81277517bee4d6b0349d71bb2661d4890b5617c" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    <link rel="stylesheet" type="text/css" href="../../_static/main.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta2/css/all.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: #4d4d4d;
  --color-brand-primary: #009191;
  --color-brand-content: #009191;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
    <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
    <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
    <header class="mobile-header">
        <div class="header-left">
            <label class="nav-overlay-icon" for="__navigation">
                <div class="visually-hidden">Toggle site navigation sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-menu"></use>
                    </svg>
                </i>
            </label>
        </div>
        <div class="header-center">
            <a href="../../">
                <div class="brand">CLIP-as-service 0.8.1 documentation</div>
            </a>
        </div>
        <div class="header-right">
            <div class="theme-toggle-container theme-toggle-header">
                <button class="theme-toggle">
                    <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                    <svg class="theme-icon-when-auto">
                        <use href="#svg-sun-half"></use>
                    </svg>
                    <svg class="theme-icon-when-dark">
                        <use href="#svg-moon"></use>
                    </svg>
                    <svg class="theme-icon-when-light">
                        <use href="#svg-sun"></use>
                    </svg>
                </button>
            </div>
            <label class="toc-overlay-icon toc-header-icon" for="__toc">
                <div class="visually-hidden">Toggle table of contents sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-toc"></use>
                    </svg>
                </i>
            </label>
        </div>
    </header>
    <aside class="sidebar-drawer">
        <div class="sidebar-container">
            
            <div class="sidebar-sticky"><a class="sidebar-brand" href="../../">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../../_static/logo-light.svg" alt="Light Logo" />
    <img class="sidebar-logo only-dark" src="../../_static/logo-dark.svg" alt="Dark Logo" />
  </div>
  
  
</a>
<div class="sd-d-flex-row sd-align-major-spaced">
  <a class="github-button" href="https://github.com/jina-ai/clip-as-service" data-icon="octicon-star" data-show-count="true" aria-label="Star jina-ai/jina on GitHub" style="opacity: 0;">Star</a>
  
</div><form class="sidebar-search-container" method="get" action="../../search/" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
    <p class="caption" role="heading"><span class="caption-text">User Guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../client/">Client API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../server/">Server API</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../retriever/">CLIP Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Hosting</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../hosting/by-jina/">Hosted by Jina AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hosting/on-jcloud/">Host on JCloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hosting/colab/">Host on Google Colab</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../playground/embedding/">Text &amp; Image Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/reasoning/">Visual Reasoning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/searching/">Text &amp; Image Searching</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer References</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api/clip_client/">clip_client package</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api/clip_client.client/">clip_client.client module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/clip_client.helper/">clip_client.helper module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog/">Changelog</a></li>
</ul>

    <p class="caption" role="heading"><span class="caption-text">Ecosystem</span></p>
    <ul>
        <li class="toctree-l1">
            <a class="reference external" href="https://docs.jina.ai">
                <img class="sidebar-ecosys-logo only-light-line" src="../../_static/search-light.svg">
                <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/search-dark.svg">
                Jina</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://hub.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/hub-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/hub-dark.svg">
            Jina Hub</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://finetuner.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/finetuner-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/finetuner-dark.svg">
            Finetuner</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://docarray.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/docarray-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/docarray-dark.svg">
            DocArray</a></li>
        <li class="toctree-l1"><a class="reference internal" href="#">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/cas-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/cas-dark.svg">
            CLIP-as-service</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://github.com/jina-ai/jcloud">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/JCloud-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/JCloud-dark.svg">
            JCloud</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://now.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/now-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/now-dark.svg">
            NOW</a></li>
    </ul>
</div>
</div>

            </div>
            
        </div>
    </aside>
    <div class="main">
        <div class="content">
            <div class="article-container">
                <a href="#" class="back-to-top muted-link">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
                    </svg>
                    <span>Back to top</span>
                </a>
                <div class="content-icon-container"><div class="theme-toggle-container theme-toggle-content">
                        <button class="theme-toggle">
                            <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                            <svg class="theme-icon-when-auto">
                                <use href="#svg-sun-half"></use>
                            </svg>
                            <svg class="theme-icon-when-dark">
                                <use href="#svg-moon"></use>
                            </svg>
                            <svg class="theme-icon-when-light">
                                <use href="#svg-sun"></use>
                            </svg>
                        </button>
                    </div>
                    <label class="toc-overlay-icon toc-content-icon"
                           for="__toc">
                        <div class="visually-hidden">Toggle table of contents sidebar</div>
                        <i class="icon">
                            <svg>
                                <use href="#svg-toc"></use>
                            </svg>
                        </i>
                    </label>
                </div>
                <article role="main">
                    <section id="benchmark">
<h1>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">#</a></h1>
<p>In order to understand the zero-shot performance of CLIP and its limitations, we conducted a benchmark
across a variety of computer vision datasets (the dataset details are in the appendix). Here, thanks for the
open-source <a class="reference external" href="https://github.com/LAION-AI/CLIP_benchmark">CLIP Benchmark toolkit</a>, we can easily reproduce the results.</p>
<p>We hope that this benchmark can help you to better understand the performance of CLIP models and choose the best model for your application.</p>
<section id="select-the-right-model">
<h2>Select the right model<a class="headerlink" href="#select-the-right-model" title="Permalink to this headline">#</a></h2>
<p>In general, you can select the best model for your application from different perspectives: disk usage, peak RAM and VRAM usages, QPS, and most importantly, the performance.</p>
<p>Based on our experiments, we recommend the ViT models over the RN models for most general applications.
More specifically, the <code class="docutils literal notranslate"><span class="pre">ViT-H-14::laion2b_s32b_b79k</span></code> model and <code class="docutils literal notranslate"><span class="pre">ViT-g-14::laion2b_s12b_b42k</span></code> model should be first considered since they have the best or close to the best performance in most cases.
However, if you are concerned about the encoding speed, you can consider other ViT models because they have higher QPS with decent performance.
Anyway, you should choose the model that best fits your requirements.
For example, if you are labeling images for diabetic retinopathy, you should probably select the <code class="docutils literal notranslate"><span class="pre">ViT-B-32::laion2b_s34b_b79k</span></code> model since it has the best top-1 accuracy of 0.734 on zero-shot classification of the Retinopathy dataset.
Or if you are dealing with histopathologic images, you should probably select the RN50::openai model since it has the best top-1 accuracy of 0.636 on zero-shot classification of the Patch Camelyon dataset.</p>
<p>The following sections show the performance of different models in details on different datasets and tasks.</p>
</section>
<section id="size-and-efficiency">
<h2>Size and efficiency<a class="headerlink" href="#size-and-efficiency" title="Permalink to this headline">#</a></h2>
<p>We first present the modelâ€™s size and efficiency in terms of query time and memory usage (including the peak RAM and VRAM usage).
All of the results are obtained on a single Nvidia TITAN RTX GPU (24GB VRAM) with default server settings.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 32%" />
<col style="width: 14%" />
<col style="width: 17%" />
<col style="width: 18%" />
<col style="width: 9%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Disk Usage (MB)</p></th>
<th class="head"><p>Peak RAM Usage (GB)</p></th>
<th class="head"><p>Peak VRAM Usage (GB)</p></th>
<th class="head"><p>Text QPS</p></th>
<th class="head"><p>Image QPS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>RN50::openai</p></td>
<td><p>244</p></td>
<td><p>2.99</p></td>
<td><p>1.36</p></td>
<td><p>1019</p></td>
<td><p>269</p></td>
</tr>
<tr class="row-odd"><td><p>RN50::yfcc15m</p></td>
<td><p>389</p></td>
<td><p>2.86</p></td>
<td><p>1.36</p></td>
<td><p>1083</p></td>
<td><p>262</p></td>
</tr>
<tr class="row-even"><td><p>RN50::cc12m</p></td>
<td><p>389</p></td>
<td><p>2.84</p></td>
<td><p>1.36</p></td>
<td><p>1064</p></td>
<td><p>264</p></td>
</tr>
<tr class="row-odd"><td><p>RN101::openai</p></td>
<td><p>278</p></td>
<td><p>3.05</p></td>
<td><p>1.40</p></td>
<td><p>1047</p></td>
<td><p>222</p></td>
</tr>
<tr class="row-even"><td><p>RN101::yfcc15m</p></td>
<td><p>457</p></td>
<td><p>2.88</p></td>
<td><p>1.40</p></td>
<td><p>1107</p></td>
<td><p>223</p></td>
</tr>
<tr class="row-odd"><td><p>RN50x4::openai</p></td>
<td><p>402</p></td>
<td><p>3.23</p></td>
<td><p>1.63</p></td>
<td><p>1047</p></td>
<td><p>218</p></td>
</tr>
<tr class="row-even"><td><p>RN50x16::openai</p></td>
<td><p>631</p></td>
<td><p>3.63</p></td>
<td><p>2.02</p></td>
<td><p>1038</p></td>
<td><p>121</p></td>
</tr>
<tr class="row-odd"><td><p>RN50x64::openai</p></td>
<td><p>1291</p></td>
<td><p>4.08</p></td>
<td><p>2.98</p></td>
<td><p>985</p></td>
<td><p>59</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::openai</p></td>
<td><p>338</p></td>
<td><p>3.20</p></td>
<td><p>1.40</p></td>
<td><p>1064</p></td>
<td><p>286</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::laion2b_e16</p></td>
<td><p>577</p></td>
<td><p>2.93</p></td>
<td><p>1.40</p></td>
<td><p>1120</p></td>
<td><p>292</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::laion400m_e31</p></td>
<td><p>577</p></td>
<td><p>2.93</p></td>
<td><p>1.40</p></td>
<td><p>1080</p></td>
<td><p>287</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::laion400m_e32</p></td>
<td><p>577</p></td>
<td><p>2.94</p></td>
<td><p>1.40</p></td>
<td><p>1092</p></td>
<td><p>289</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::laion2b-s34b-b79k</p></td>
<td><p>577</p></td>
<td><p>2.94</p></td>
<td><p>1.40</p></td>
<td><p>1102</p></td>
<td><p>285</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16::openai</p></td>
<td><p>335</p></td>
<td><p>3.20</p></td>
<td><p>1.44</p></td>
<td><p>1064</p></td>
<td><p>260</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16::laion400m_e31</p></td>
<td><p>571</p></td>
<td><p>2.93</p></td>
<td><p>1.44</p></td>
<td><p>1099</p></td>
<td><p>262</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16::laion400m_e32</p></td>
<td><p>571</p></td>
<td><p>2.94</p></td>
<td><p>1.44</p></td>
<td><p>1082</p></td>
<td><p>268</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16-plus-240::laion400m_e31</p></td>
<td><p>795</p></td>
<td><p>3.03</p></td>
<td><p>1.59</p></td>
<td><p>1059</p></td>
<td><p>235</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16-plus-240::laion400m_e32</p></td>
<td><p>795</p></td>
<td><p>3.03</p></td>
<td><p>1.59</p></td>
<td><p>1043</p></td>
<td><p>239</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14::openai</p></td>
<td><p>890</p></td>
<td><p>3.66</p></td>
<td><p>2.04</p></td>
<td><p>1040</p></td>
<td><p>140</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14::laion400m_e31</p></td>
<td><p>1631</p></td>
<td><p>3.43</p></td>
<td><p>2.03</p></td>
<td><p>1058</p></td>
<td><p>147</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14::laion400m_e32</p></td>
<td><p>1631</p></td>
<td><p>3.42</p></td>
<td><p>2.03</p></td>
<td><p>1061</p></td>
<td><p>146</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14::laion2b-s32b-b82k</p></td>
<td><p>1631</p></td>
<td><p>3.43</p></td>
<td><p>2.03</p></td>
<td><p>1069</p></td>
<td><p>147</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14-336::openai</p></td>
<td><p>891</p></td>
<td><p>3.74</p></td>
<td><p>2.23</p></td>
<td><p>1070</p></td>
<td><p>76</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-H-14::laion2b-s32b-b79k</p></td>
<td><p>3762</p></td>
<td><p>4.45</p></td>
<td><p>3.26</p></td>
<td><p>642</p></td>
<td><p>91</p></td>
</tr>
<tr class="row-even"><td><p>ViT-g-14::laion2b-s12b-b42k</p></td>
<td><p>5214</p></td>
<td><p>5.16</p></td>
<td><p>4.00</p></td>
<td><p>639</p></td>
<td><p>69</p></td>
</tr>
<tr class="row-odd"><td><p>M-CLIP/LABSE-Vit-L-14</p></td>
<td><p>3609</p></td>
<td><p>4.30</p></td>
<td><p>4.70</p></td>
<td><p>646</p></td>
<td><p>284</p></td>
</tr>
<tr class="row-even"><td><p>M-CLIP/XLM-Roberta-Large-Vit-B-32</p></td>
<td><p>4284</p></td>
<td><p>5.37</p></td>
<td><p>1.68</p></td>
<td><p>656</p></td>
<td><p>139</p></td>
</tr>
<tr class="row-odd"><td><p>M-CLIP/XLM-Roberta-Large-Vit-B-16Plus</p></td>
<td><p>4293</p></td>
<td><p>4.30</p></td>
<td><p>4.13</p></td>
<td><p>662</p></td>
<td><p>236</p></td>
</tr>
<tr class="row-even"><td><p>M-CLIP/XLM-Roberta-Large-Vit-L-14</p></td>
<td><p>4293</p></td>
<td><p>4.30</p></td>
<td><p>4.97</p></td>
<td><p>1027</p></td>
<td><p>139</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="zero-shot-performance">
<h2>Zero-shot performance<a class="headerlink" href="#zero-shot-performance" title="Permalink to this headline">#</a></h2>
<p>In this section, we will report the zero-shot performance of the models on classification and retrieval tasks across different datasets.
In the following tables, we will highlight the best results in bold for each dataset (higher is better).</p>
<section id="zero-shot-retrieval">
<h3>Zero-shot retrieval<a class="headerlink" href="#zero-shot-retrieval" title="Permalink to this headline">#</a></h3>
<p>In zero-shot retrieval benchmark, each model is evaluated on the following datasets: <a class="reference external" href="https://github.com/tylin/coco-caption">COCO Caption</a>, <a class="reference external" href="http://hockenmaier.cs.illinois.edu/8k-pictures.html">Flickr8k</a> and <a class="reference external" href="https://shannon.cs.illinois.edu/DenotationGraph/">Flickr30k</a>.
For the above datasets, there are five corresponding description sentences for each image written by humans.
The results are reported in terms of top-5 text-to-image retrieval recall, top-5 image-to-text retrieval recall and their averages.
More specifically, the top-5 text-to-image retrieval recall for each retrieved image is either 1 or 0.
It is 1 if the input text matches one of the image descriptions among the top-5.
The top-5 image-to-text retrieval recall for each image is the number of top-5 retrieved texts matching that image descriptions.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 7%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 7%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" rowspan="2"><p>Model</p></th>
<th class="head" colspan="3"><p>COCO Caption</p></th>
<th class="head" colspan="3"><p>Flickr 8k</p></th>
<th class="head" colspan="3"><p>Flickr 30k</p></th>
</tr>
<tr class="row-even"><th class="head"><p>Text to image</p></th>
<th class="head"><p>Image to text</p></th>
<th class="head"><p>Average</p></th>
<th class="head"><p>Text to image</p></th>
<th class="head"><p>Image to text</p></th>
<th class="head"><p>Average</p></th>
<th class="head"><p>Text to image</p></th>
<th class="head"><p>Image to text</p></th>
<th class="head"><p>Average</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>RN50::openai</p></td>
<td><p>0.529</p></td>
<td><p>0.728</p></td>
<td><p>0.629</p></td>
<td><p>0.504</p></td>
<td><p>0.690</p></td>
<td><p>0.597</p></td>
<td><p>0.392</p></td>
<td><p>0.621</p></td>
<td><p>0.506</p></td>
</tr>
<tr class="row-even"><td><p>RN50::yfcc15m</p></td>
<td><p>0.361</p></td>
<td><p>0.534</p></td>
<td><p>0.447</p></td>
<td><p>0.238</p></td>
<td><p>0.394</p></td>
<td><p>0.316</p></td>
<td><p>0.146</p></td>
<td><p>0.278</p></td>
<td><p>0.212</p></td>
</tr>
<tr class="row-odd"><td><p>RN50::cc12m</p></td>
<td><p>0.446</p></td>
<td><p>0.607</p></td>
<td><p>0.527</p></td>
<td><p>0.302</p></td>
<td><p>0.435</p></td>
<td><p>0.369</p></td>
<td><p>0.204</p></td>
<td><p>0.316</p></td>
<td><p>0.260</p></td>
</tr>
<tr class="row-even"><td><p>RN101::openai</p></td>
<td><p>0.555</p></td>
<td><p>0.745</p></td>
<td><p>0.650</p></td>
<td><p>0.523</p></td>
<td><p>0.694</p></td>
<td><p>0.608</p></td>
<td><p>0.415</p></td>
<td><p>0.629</p></td>
<td><p>0.522</p></td>
</tr>
<tr class="row-odd"><td><p>RN101::yfcc15m</p></td>
<td><p>0.376</p></td>
<td><p>0.549</p></td>
<td><p>0.463</p></td>
<td><p>0.251</p></td>
<td><p>0.417</p></td>
<td><p>0.334</p></td>
<td><p>0.156</p></td>
<td><p>0.296</p></td>
<td><p>0.226</p></td>
</tr>
<tr class="row-even"><td><p>RN50x4::openai</p></td>
<td><p>0.581</p></td>
<td><p>0.767</p></td>
<td><p>0.674</p></td>
<td><p>0.558</p></td>
<td><p>0.729</p></td>
<td><p>0.643</p></td>
<td><p>0.451</p></td>
<td><p>0.671</p></td>
<td><p>0.561</p></td>
</tr>
<tr class="row-odd"><td><p>RN50x16::openai</p></td>
<td><p>0.600</p></td>
<td><p>0.787</p></td>
<td><p>0.693</p></td>
<td><p>0.597</p></td>
<td><p>0.768</p></td>
<td><p>0.682</p></td>
<td><p>0.496</p></td>
<td><p>0.713</p></td>
<td><p>0.604</p></td>
</tr>
<tr class="row-even"><td><p>RN50x64::openai</p></td>
<td><p>0.599</p></td>
<td><p>0.803</p></td>
<td><p>0.701</p></td>
<td><p>0.629</p></td>
<td><p>0.790</p></td>
<td><p>0.709</p></td>
<td><p>0.534</p></td>
<td><p>0.756</p></td>
<td><p>0.645</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::openai</p></td>
<td><p>0.560</p></td>
<td><p>0.749</p></td>
<td><p>0.654</p></td>
<td><p>0.532</p></td>
<td><p>0.699</p></td>
<td><p>0.616</p></td>
<td><p>0.413</p></td>
<td><p>0.629</p></td>
<td><p>0.521</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::laion2b_e16</p></td>
<td><p>0.647</p></td>
<td><p>0.795</p></td>
<td><p>0.721</p></td>
<td><p>0.622</p></td>
<td><p>0.760</p></td>
<td><p>0.691</p></td>
<td><p>0.507</p></td>
<td><p>0.687</p></td>
<td><p>0.597</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::laion400m_e31</p></td>
<td><p>0.600</p></td>
<td><p>0.763</p></td>
<td><p>0.682</p></td>
<td><p>0.562</p></td>
<td><p>0.736</p></td>
<td><p>0.649</p></td>
<td><p>0.438</p></td>
<td><p>0.633</p></td>
<td><p>0.536</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::laion400m_e32</p></td>
<td><p>0.600</p></td>
<td><p>0.765</p></td>
<td><p>0.682</p></td>
<td><p>0.562</p></td>
<td><p>0.736</p></td>
<td><p>0.649</p></td>
<td><p>0.437</p></td>
<td><p>0.634</p></td>
<td><p>0.536</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::laion2b_s34b_b79k</p></td>
<td><p>0.654</p></td>
<td><p>0.798</p></td>
<td><p>0.726</p></td>
<td><p>0.629</p></td>
<td><p>0.778</p></td>
<td><p>0.703</p></td>
<td><p>0.513</p></td>
<td><p>0.694</p></td>
<td><p>0.603</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16::openai</p></td>
<td><p>0.584</p></td>
<td><p>0.767</p></td>
<td><p>0.676</p></td>
<td><p>0.564</p></td>
<td><p>0.727</p></td>
<td><p>0.646</p></td>
<td><p>0.452</p></td>
<td><p>0.671</p></td>
<td><p>0.561</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16::laion400m_e31</p></td>
<td><p>0.637</p></td>
<td><p>0.796</p></td>
<td><p>0.717</p></td>
<td><p>0.620</p></td>
<td><p>0.765</p></td>
<td><p>0.692</p></td>
<td><p>0.506</p></td>
<td><p>0.697</p></td>
<td><p>0.602</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16::laion400m_e32</p></td>
<td><p>0.636</p></td>
<td><p>0.796</p></td>
<td><p>0.716</p></td>
<td><p>0.620</p></td>
<td><p>0.767</p></td>
<td><p>0.694</p></td>
<td><p>0.508</p></td>
<td><p>0.697</p></td>
<td><p>0.603</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16-plus-240::laion400m_e31</p></td>
<td><p>0.660</p></td>
<td><p>0.809</p></td>
<td><p>0.735</p></td>
<td><p>0.642</p></td>
<td><p>0.788</p></td>
<td><p>0.715</p></td>
<td><p>0.533</p></td>
<td><p>0.725</p></td>
<td><p>0.629</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16-plus-240::laion400m_e32</p></td>
<td><p>0.662</p></td>
<td><p>0.811</p></td>
<td><p>0.736</p></td>
<td><p>0.644</p></td>
<td><p>0.791</p></td>
<td><p>0.718</p></td>
<td><p>0.535</p></td>
<td><p>0.727</p></td>
<td><p>0.631</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14::openai</p></td>
<td><p>0.610</p></td>
<td><p>0.793</p></td>
<td><p>0.702</p></td>
<td><p>0.599</p></td>
<td><p>0.767</p></td>
<td><p>0.683</p></td>
<td><p>0.494</p></td>
<td><p>0.717</p></td>
<td><p>0.605</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14::laion400m_e31</p></td>
<td><p>0.680</p></td>
<td><p>0.821</p></td>
<td><p>0.750</p></td>
<td><p>0.675</p></td>
<td><p>0.806</p></td>
<td><p>0.741</p></td>
<td><p>0.570</p></td>
<td><p>0.751</p></td>
<td><p>0.661</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14::laion400m_e32</p></td>
<td><p>0.680</p></td>
<td><p>0.821</p></td>
<td><p>0.751</p></td>
<td><p>0.675</p></td>
<td><p>0.806</p></td>
<td><p>0.740</p></td>
<td><p>0.570</p></td>
<td><p>0.751</p></td>
<td><p>0.661</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14::laion2b_s32b_b82k</p></td>
<td><p>0.711</p></td>
<td><p>0.840</p></td>
<td><p>0.775</p></td>
<td><p>0.712</p></td>
<td><p>0.824</p></td>
<td><p>0.768</p></td>
<td><p>0.620</p></td>
<td><p>0.789</p></td>
<td><p>0.704</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14-336::openai</p></td>
<td><p>0.616</p></td>
<td><p>0.812</p></td>
<td><p>0.714</p></td>
<td><p>0.629</p></td>
<td><p>0.779</p></td>
<td><p>0.704</p></td>
<td><p>0.533</p></td>
<td><p>0.741</p></td>
<td><p>0.637</p></td>
</tr>
<tr class="row-even"><td><p>ViT-H-14::laion2b_s32b_b79k</p></td>
<td><p><strong>0.734</strong></p></td>
<td><p><strong>0.861</strong></p></td>
<td><p><strong>0.797</strong></p></td>
<td><p><strong>0.746</strong></p></td>
<td><p><strong>0.856</strong></p></td>
<td><p><strong>0.801</strong></p></td>
<td><p><strong>0.657</strong></p></td>
<td><p><strong>0.823</strong></p></td>
<td><p><strong>0.740</strong></p></td>
</tr>
<tr class="row-odd"><td><p>ViT-g-14::laion2b_s12b_b42k</p></td>
<td><p>0.724</p></td>
<td><p>0.853</p></td>
<td><p>0.788</p></td>
<td><p>0.730</p></td>
<td><p>0.846</p></td>
<td><p>0.788</p></td>
<td><p>0.639</p></td>
<td><p>0.806</p></td>
<td><p>0.722</p></td>
</tr>
</tbody>
</table>
</div>
<p>From the table, we observe that the ViT models outperform the RN models in general.
More specifically, the <code class="docutils literal notranslate"><span class="pre">ViT-H-14::laion2b_s32b_b79k</span></code> model and <code class="docutils literal notranslate"><span class="pre">ViT-g-14::laion2b_s12b_b42k</span></code> model achieve the best and second-best results on all zero-shot retrieval tasks.
For ViT models, the results of the same base model are better on those pre-trained with larger datasets (e.g., <code class="docutils literal notranslate"><span class="pre">ViT-B-32::openai</span></code> vs <code class="docutils literal notranslate"><span class="pre">ViT-B-32::laion400m_e31</span></code> vs <code class="docutils literal notranslate"><span class="pre">ViT-B-32::laion2b-s34b-b79k</span></code>).</p>
</section>
<section id="zero-shot-classification">
<h3>Zero-shot classification<a class="headerlink" href="#zero-shot-classification" title="Permalink to this headline">#</a></h3>
<p>In zero-shot classification benchmark, each model is evaluated on the following datasets: <a class="reference external" href="https://github.com/modestyachts/ImageNetV2">ImageNetV2</a>, <a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/">VOC2007</a> and 19 <a class="reference external" href="https://github.com/google-research/task_adaptation">VTAB datasets</a>.
The results are shown in the following table.
For each dataset, we report the top-1 accuracy, which is whether the top-1 retrieved class of a image matches its true class.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 4%" />
<col style="width: 3%" />
<col style="width: 4%" />
<col style="width: 3%" />
<col style="width: 3%" />
<col style="width: 4%" />
<col style="width: 3%" />
<col style="width: 3%" />
<col style="width: 3%" />
<col style="width: 3%" />
<col style="width: 3%" />
<col style="width: 5%" />
<col style="width: 4%" />
<col style="width: 4%" />
<col style="width: 5%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 3%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" rowspan="2"><p>Model</p></th>
<th class="head" rowspan="2"><p>ImageNetV2</p></th>
<th class="head" rowspan="2"><p>VOC2007</p></th>
<th class="head" colspan="7"><p>VTAB natural</p></th>
<th class="head" colspan="4"><p>VTAB specialized</p></th>
<th class="head" colspan="8"><p>VTAB structured</p></th>
</tr>
<tr class="row-even"><th class="head"><p>Caltech101</p></th>
<th class="head"><p>CIFAR-100</p></th>
<th class="head"><p>DTD</p></th>
<th class="head"><p>Flowers102</p></th>
<th class="head"><p>Pets</p></th>
<th class="head"><p>Sun397</p></th>
<th class="head"><p>SVHN</p></th>
<th class="head"><p>EuroSAT</p></th>
<th class="head"><p>Resisc45</p></th>
<th class="head"><p>Patch Camelyon</p></th>
<th class="head"><p>Retinopathy</p></th>
<th class="head"><p>Clevr/count</p></th>
<th class="head"><p>Clevr/distance</p></th>
<th class="head"><p>dSprites/location</p></th>
<th class="head"><p>dSprites/orientation</p></th>
<th class="head"><p>SmallNORB/azimuth</p></th>
<th class="head"><p>SmallNORB/elevation</p></th>
<th class="head"><p>DMLab</p></th>
<th class="head"><p>KITTI/distance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>RN50::openai</p></td>
<td><p>0.529</p></td>
<td><p>0.650</p></td>
<td><p>0.772</p></td>
<td><p>0.403</p></td>
<td><p>0.415</p></td>
<td><p>0.660</p></td>
<td><p>0.857</p></td>
<td><p>0.894</p></td>
<td><p>0.303</p></td>
<td><p>0.408</p></td>
<td><p>0.453</p></td>
<td><p><strong>0.636</strong></p></td>
<td><p>0.171</p></td>
<td><p>0.217</p></td>
<td><p>0.148</p></td>
<td><p>0.034</p></td>
<td><p>0.014</p></td>
<td><p>0.056</p></td>
<td><p>0.110</p></td>
<td><p>0.145</p></td>
<td><p>0.170</p></td>
</tr>
<tr class="row-even"><td><p>RN50::yfcc15m</p></td>
<td><p>0.214</p></td>
<td><p>0.215</p></td>
<td><p>0.402</p></td>
<td><p>0.116</p></td>
<td><p>0.122</p></td>
<td><p>0.167</p></td>
<td><p>0.174</p></td>
<td><p>0.127</p></td>
<td><p>0.157</p></td>
<td><p>0.172</p></td>
<td><p>0.123</p></td>
<td><p>0.533</p></td>
<td><p>0.358</p></td>
<td><p>0.151</p></td>
<td><p>0.158</p></td>
<td><p>0.032</p></td>
<td><p>0.024</p></td>
<td><p>0.053</p></td>
<td><p>0.120</p></td>
<td><p>0.160</p></td>
<td><p><strong>0.336</strong></p></td>
</tr>
<tr class="row-odd"><td><p>RN50::cc12m</p></td>
<td><p>0.224</p></td>
<td><p>0.438</p></td>
<td><p>0.582</p></td>
<td><p>0.178</p></td>
<td><p>0.135</p></td>
<td><p>0.095</p></td>
<td><p>0.331</p></td>
<td><p>0.123</p></td>
<td><p>0.102</p></td>
<td><p>0.148</p></td>
<td><p>0.117</p></td>
<td><p>0.535</p></td>
<td><p>0.293</p></td>
<td><p>0.184</p></td>
<td><p>0.222</p></td>
<td><p>0.031</p></td>
<td><p>0.025</p></td>
<td><p>0.047</p></td>
<td><p>0.096</p></td>
<td><p>0.161</p></td>
<td><p>0.155</p></td>
</tr>
<tr class="row-even"><td><p>RN101::openai</p></td>
<td><p>0.561</p></td>
<td><p>0.651</p></td>
<td><p>0.780</p></td>
<td><p>0.476</p></td>
<td><p>0.432</p></td>
<td><p>0.652</p></td>
<td><p>0.869</p></td>
<td><p>0.887</p></td>
<td><p>0.226</p></td>
<td><p>0.314</p></td>
<td><p>0.547</p></td>
<td><p>0.583</p></td>
<td><p>0.280</p></td>
<td><p>0.242</p></td>
<td><p>0.130</p></td>
<td><p>0.031</p></td>
<td><p>0.021</p></td>
<td><p>0.054</p></td>
<td><p>0.111</p></td>
<td><p>0.139</p></td>
<td><p>0.263</p></td>
</tr>
<tr class="row-odd"><td><p>RN101::yfcc15m</p></td>
<td><p>0.221</p></td>
<td><p>0.243</p></td>
<td><p>0.469</p></td>
<td><p>0.125</p></td>
<td><p>0.117</p></td>
<td><p>0.210</p></td>
<td><p>0.177</p></td>
<td><p>0.128</p></td>
<td><p>0.137</p></td>
<td><p>0.151</p></td>
<td><p>0.099</p></td>
<td><p>0.479</p></td>
<td><p>0.584</p></td>
<td><p>0.109</p></td>
<td><p>0.159</p></td>
<td><p>0.031</p></td>
<td><p>0.019</p></td>
<td><p>0.055</p></td>
<td><p>0.097</p></td>
<td><p>0.153</p></td>
<td><p>0.252</p></td>
</tr>
<tr class="row-even"><td><p>RN50x4::openai</p></td>
<td><p>0.594</p></td>
<td><p>0.682</p></td>
<td><p>0.781</p></td>
<td><p>0.451</p></td>
<td><p>0.486</p></td>
<td><p>0.698</p></td>
<td><p>0.887</p></td>
<td><p>0.908</p></td>
<td><p>0.367</p></td>
<td><p>0.335</p></td>
<td><p>0.532</p></td>
<td><p>0.569</p></td>
<td><p>0.318</p></td>
<td><p>0.205</p></td>
<td><p>0.082</p></td>
<td><p>0.031</p></td>
<td><p>0.026</p></td>
<td><p>0.056</p></td>
<td><p>0.108</p></td>
<td><p>0.162</p></td>
<td><p>0.233</p></td>
</tr>
<tr class="row-odd"><td><p>RN50x16::openai</p></td>
<td><p>0.643</p></td>
<td><p>0.680</p></td>
<td><p>0.810</p></td>
<td><p>0.522</p></td>
<td><p>0.524</p></td>
<td><p>0.724</p></td>
<td><p>0.898</p></td>
<td><p>0.917</p></td>
<td><p>0.409</p></td>
<td><p>0.433</p></td>
<td><p>0.589</p></td>
<td><p>0.625</p></td>
<td><p>0.715</p></td>
<td><p>0.195</p></td>
<td><p>0.213</p></td>
<td><p>0.030</p></td>
<td><p>0.026</p></td>
<td><p>0.050</p></td>
<td><p>0.116</p></td>
<td><p>0.146</p></td>
<td><p>0.229</p></td>
</tr>
<tr class="row-even"><td><p>RN50x64::openai</p></td>
<td><p>0.670</p></td>
<td><p>0.740</p></td>
<td><p>0.834</p></td>
<td><p>0.598</p></td>
<td><p>0.531</p></td>
<td><p>0.788</p></td>
<td><p>0.936</p></td>
<td><p>0.931</p></td>
<td><p>0.481</p></td>
<td><p>0.577</p></td>
<td><p>0.628</p></td>
<td><p>0.539</p></td>
<td><p>0.073</p></td>
<td><p>0.227</p></td>
<td><p>0.200</p></td>
<td><p>0.034</p></td>
<td><p>0.025</p></td>
<td><p>0.056</p></td>
<td><p>0.125</p></td>
<td><p>0.158</p></td>
<td><p>0.311</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::openai</p></td>
<td><p>0.559</p></td>
<td><p>0.764</p></td>
<td><p>0.815</p></td>
<td><p>0.643</p></td>
<td><p>0.443</p></td>
<td><p>0.664</p></td>
<td><p>0.873</p></td>
<td><p>0.913</p></td>
<td><p>0.135</p></td>
<td><p>0.504</p></td>
<td><p>0.537</p></td>
<td><p>0.623</p></td>
<td><p>0.447</p></td>
<td><p>0.232</p></td>
<td><p>0.164</p></td>
<td><p>0.037</p></td>
<td><p>0.024</p></td>
<td><p>0.061</p></td>
<td><p><strong>0.127</strong></p></td>
<td><p>0.193</p></td>
<td><p>0.274</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::laion2b_e16</p></td>
<td><p>0.573</p></td>
<td><p>0.788</p></td>
<td><p>0.831</p></td>
<td><p>0.754</p></td>
<td><p>0.539</p></td>
<td><p>0.691</p></td>
<td><p>0.893</p></td>
<td><p>0.933</p></td>
<td><p>0.388</p></td>
<td><p>0.503</p></td>
<td><p>0.619</p></td>
<td><p>0.506</p></td>
<td><p>0.195</p></td>
<td><p>0.192</p></td>
<td><p>0.167</p></td>
<td><p>0.031</p></td>
<td><p>0.024</p></td>
<td><p>0.052</p></td>
<td><p>0.110</p></td>
<td><p>0.189</p></td>
<td><p>0.176</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::laion400m_e31</p></td>
<td><p>0.523</p></td>
<td><p>0.731</p></td>
<td><p>0.818</p></td>
<td><p>0.678</p></td>
<td><p>0.521</p></td>
<td><p>0.659</p></td>
<td><p>0.856</p></td>
<td><p>0.918</p></td>
<td><p>0.220</p></td>
<td><p>0.470</p></td>
<td><p>0.510</p></td>
<td><p>0.549</p></td>
<td><p>0.259</p></td>
<td><p>0.155</p></td>
<td><p>0.161</p></td>
<td><p>0.033</p></td>
<td><p>0.021</p></td>
<td><p>0.053</p></td>
<td><p>0.117</p></td>
<td><p>0.173</p></td>
<td><p>0.122</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-32::laion400m_e32</p></td>
<td><p>0.523</p></td>
<td><p>0.733</p></td>
<td><p>0.817</p></td>
<td><p>0.677</p></td>
<td><p>0.523</p></td>
<td><p>0.658</p></td>
<td><p>0.854</p></td>
<td><p>0.917</p></td>
<td><p>0.223</p></td>
<td><p>0.476</p></td>
<td><p>0.510</p></td>
<td><p>0.548</p></td>
<td><p>0.240</p></td>
<td><p>0.153</p></td>
<td><p>0.161</p></td>
<td><p>0.033</p></td>
<td><p>0.021</p></td>
<td><p>0.054</p></td>
<td><p>0.117</p></td>
<td><p>0.173</p></td>
<td><p>0.118</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-32::laion2b_s34b_b79k</p></td>
<td><p>0.581</p></td>
<td><p>0.791</p></td>
<td><p>0.839</p></td>
<td><p>0.755</p></td>
<td><p>0.557</p></td>
<td><p>0.716</p></td>
<td><p>0.909</p></td>
<td><p>0.937</p></td>
<td><p>0.410</p></td>
<td><p>0.482</p></td>
<td><p>0.610</p></td>
<td><p>0.598</p></td>
<td><p><strong>0.734</strong></p></td>
<td><p>0.153</p></td>
<td><p>0.189</p></td>
<td><p>0.029</p></td>
<td><p><strong>0.034</strong></p></td>
<td><p><strong>0.062</strong></p></td>
<td><p>0.113</p></td>
<td><p>0.159</p></td>
<td><p>0.262</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16::openai</p></td>
<td><p>0.619</p></td>
<td><p>0.783</p></td>
<td><p>0.819</p></td>
<td><p>0.669</p></td>
<td><p>0.449</p></td>
<td><p>0.712</p></td>
<td><p>0.890</p></td>
<td><p>0.924</p></td>
<td><p>0.313</p></td>
<td><p>0.559</p></td>
<td><p>0.582</p></td>
<td><p>0.507</p></td>
<td><p>0.036</p></td>
<td><p>0.209</p></td>
<td><p>0.158</p></td>
<td><p>0.030</p></td>
<td><p>0.023</p></td>
<td><p>0.053</p></td>
<td><p>0.122</p></td>
<td><p>0.155</p></td>
<td><p>0.263</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16::laion400m_e31</p></td>
<td><p>0.594</p></td>
<td><p>0.767</p></td>
<td><p>0.838</p></td>
<td><p>0.712</p></td>
<td><p>0.513</p></td>
<td><p>0.694</p></td>
<td><p>0.892</p></td>
<td><p>0.939</p></td>
<td><p>0.380</p></td>
<td><p>0.503</p></td>
<td><p>0.585</p></td>
<td><p>0.593</p></td>
<td><p>0.062</p></td>
<td><p>0.289</p></td>
<td><p><strong>0.245</strong></p></td>
<td><p>0.031</p></td>
<td><p>0.030</p></td>
<td><p>0.059</p></td>
<td><p>0.100</p></td>
<td><p>0.152</p></td>
<td><p>0.200</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16::laion400m_e32</p></td>
<td><p>0.597</p></td>
<td><p>0.768</p></td>
<td><p>0.837</p></td>
<td><p>0.712</p></td>
<td><p>0.513</p></td>
<td><p>0.692</p></td>
<td><p>0.892</p></td>
<td><p>0.939</p></td>
<td><p>0.385</p></td>
<td><p>0.501</p></td>
<td><p>0.585</p></td>
<td><p>0.598</p></td>
<td><p>0.077</p></td>
<td><p>0.287</p></td>
<td><p><strong>0.245</strong></p></td>
<td><p>0.032</p></td>
<td><p>0.029</p></td>
<td><p>0.060</p></td>
<td><p>0.099</p></td>
<td><p>0.151</p></td>
<td><p>0.183</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-B-16-plus-240::laion400m_e31</p></td>
<td><p>0.614</p></td>
<td><p>0.764</p></td>
<td><p>0.832</p></td>
<td><p>0.733</p></td>
<td><p>0.555</p></td>
<td><p>0.706</p></td>
<td><p>0.904</p></td>
<td><p>0.940</p></td>
<td><p>0.355</p></td>
<td><p>0.569</p></td>
<td><p>0.615</p></td>
<td><p>0.551</p></td>
<td><p>0.093</p></td>
<td><p>0.240</p></td>
<td><p>0.159</p></td>
<td><p>0.041</p></td>
<td><p>0.026</p></td>
<td><p>0.056</p></td>
<td><p>0.111</p></td>
<td><p>0.149</p></td>
<td><p>0.280</p></td>
</tr>
<tr class="row-even"><td><p>ViT-B-16-plus-240::laion400m_e32</p></td>
<td><p>0.615</p></td>
<td><p>0.764</p></td>
<td><p>0.833</p></td>
<td><p>0.738</p></td>
<td><p>0.555</p></td>
<td><p>0.711</p></td>
<td><p>0.902</p></td>
<td><p>0.940</p></td>
<td><p>0.362</p></td>
<td><p>0.581</p></td>
<td><p>0.613</p></td>
<td><p>0.551</p></td>
<td><p>0.095</p></td>
<td><p>0.238</p></td>
<td><p>0.160</p></td>
<td><p><strong>0.043</strong></p></td>
<td><p>0.027</p></td>
<td><p>0.054</p></td>
<td><p>0.110</p></td>
<td><p>0.148</p></td>
<td><p>0.281</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14::openai</p></td>
<td><p>0.698</p></td>
<td><p>0.783</p></td>
<td><p>0.835</p></td>
<td><p>0.758</p></td>
<td><p>0.554</p></td>
<td><p>0.792</p></td>
<td><p>0.932</p></td>
<td><p>0.937</p></td>
<td><p>0.571</p></td>
<td><p>0.626</p></td>
<td><p>0.633</p></td>
<td><p>0.520</p></td>
<td><p>0.733</p></td>
<td><p>0.194</p></td>
<td><p>0.161</p></td>
<td><p>0.032</p></td>
<td><p>0.023</p></td>
<td><p>0.045</p></td>
<td><p>0.115</p></td>
<td><p>0.163</p></td>
<td><p>0.218</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14::laion400m_e31</p></td>
<td><p>0.654</p></td>
<td><p>0.758</p></td>
<td><p>0.839</p></td>
<td><p>0.774</p></td>
<td><p>0.598</p></td>
<td><p>0.757</p></td>
<td><p>0.917</p></td>
<td><p>0.950</p></td>
<td><p>0.378</p></td>
<td><p>0.632</p></td>
<td><p>0.671</p></td>
<td><p>0.487</p></td>
<td><p>0.058</p></td>
<td><p>0.242</p></td>
<td><p>0.149</p></td>
<td><p>0.030</p></td>
<td><p>0.026</p></td>
<td><p>0.053</p></td>
<td><p>0.109</p></td>
<td><p>0.186</p></td>
<td><p>0.200</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14::laion400m_e32</p></td>
<td><p>0.654</p></td>
<td><p>0.756</p></td>
<td><p>0.839</p></td>
<td><p>0.774</p></td>
<td><p>0.605</p></td>
<td><p>0.756</p></td>
<td><p>0.919</p></td>
<td><p>0.950</p></td>
<td><p>0.380</p></td>
<td><p>0.622</p></td>
<td><p>0.675</p></td>
<td><p>0.493</p></td>
<td><p>0.061</p></td>
<td><p>0.243</p></td>
<td><p>0.149</p></td>
<td><p>0.030</p></td>
<td><p>0.026</p></td>
<td><p>0.053</p></td>
<td><p>0.110</p></td>
<td><p>0.186</p></td>
<td><p>0.203</p></td>
</tr>
<tr class="row-even"><td><p>ViT-L-14::laion2b_s32b_b82k</p></td>
<td><p>0.677</p></td>
<td><p>0.805</p></td>
<td><p><strong>0.851</strong></p></td>
<td><p>0.833</p></td>
<td><p>0.629</p></td>
<td><p>0.758</p></td>
<td><p>0.932</p></td>
<td><p>0.958</p></td>
<td><p>0.459</p></td>
<td><p>0.646</p></td>
<td><p>0.668</p></td>
<td><p>0.563</p></td>
<td><p>0.116</p></td>
<td><p>0.312</p></td>
<td><p>0.161</p></td>
<td><p>0.032</p></td>
<td><p>0.020</p></td>
<td><p>0.056</p></td>
<td><p>0.108</p></td>
<td><p><strong>0.224</strong></p></td>
<td><p>0.229</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-L-14-336::openai</p></td>
<td><p><strong>0.709</strong></p></td>
<td><p>0.781</p></td>
<td><p>0.837</p></td>
<td><p>0.744</p></td>
<td><p>0.556</p></td>
<td><p>0.783</p></td>
<td><p>0.937</p></td>
<td><p>0.940</p></td>
<td><p>0.560</p></td>
<td><p>0.615</p></td>
<td><p>0.638</p></td>
<td><p>0.608</p></td>
<td><p>0.733</p></td>
<td><p>0.200</p></td>
<td><p>0.158</p></td>
<td><p>0.032</p></td>
<td><p>0.024</p></td>
<td><p>0.046</p></td>
<td><p>0.113</p></td>
<td><p>0.158</p></td>
<td><p>0.262</p></td>
</tr>
<tr class="row-even"><td><p>ViT-H-14::laion2b_s32b_b79k</p></td>
<td><p><strong>0.709</strong></p></td>
<td><p>0.777</p></td>
<td><p>0.850</p></td>
<td><p><strong>0.847</strong></p></td>
<td><p>0.678</p></td>
<td><p><strong>0.801</strong></p></td>
<td><p><strong>0.945</strong></p></td>
<td><p>0.961</p></td>
<td><p>0.563</p></td>
<td><p><strong>0.726</strong></p></td>
<td><p>0.699</p></td>
<td><p>0.542</p></td>
<td><p>0.297</p></td>
<td><p>0.268</p></td>
<td><p>0.169</p></td>
<td><p>0.032</p></td>
<td><p>0.027</p></td>
<td><p>0.054</p></td>
<td><p>0.111</p></td>
<td><p>0.140</p></td>
<td><p>0.110</p></td>
</tr>
<tr class="row-odd"><td><p>ViT-g-14::laion2b_s12b_b42k</p></td>
<td><p>0.696</p></td>
<td><p><strong>0.811</strong></p></td>
<td><p><strong>0.851</strong></p></td>
<td><p>0.839</p></td>
<td><p><strong>0.682</strong></p></td>
<td><p>0.776</p></td>
<td><p>0.943</p></td>
<td><p><strong>0.962</strong></p></td>
<td><p><strong>0.603</strong></p></td>
<td><p>0.648</p></td>
<td><p>0.718</p></td>
<td><p>0.560</p></td>
<td><p>0.580</p></td>
<td><p><strong>0.332</strong></p></td>
<td><p>0.175</p></td>
<td><p>0.036</p></td>
<td><p>0.031</p></td>
<td><p>0.060</p></td>
<td><p>0.115</p></td>
<td><p>0.190</p></td>
<td><p>0.138</p></td>
</tr>
</tbody>
</table>
</div>
<p>From the table, we observe that the ViT models still outperform the RN models in most tasks, except for the Patch Camelyon dataset where <code class="docutils literal notranslate"><span class="pre">RN50::openai</span></code> has the best top-1 accuracy of 0.636, and the KITTI/distance dataset where <code class="docutils literal notranslate"><span class="pre">RN50::yfcc15m</span></code> has the best result of 0.336.
Similar to retrieval results, the <code class="docutils literal notranslate"><span class="pre">ViT-H-14::laion2b_s32b_b79k</span></code> model and <code class="docutils literal notranslate"><span class="pre">ViT-g-14::laion2b_s12b_b42k</span></code> model still have the best or close to the best results on 12/21 zero-shot classification tasks.
All models tend to perform well on ImageNetV2, VOC2007, VTAB natural and VTAB specialized (except for Retinopathy) datasets, whereas they perform poorly on VTAB structured datasets.
We do not observe any significant difference between the ViT models of the same base model.</p>
</section>
</section>
<section id="appendix-datasets-description">
<h2>Appendix: Datasets description<a class="headerlink" href="#appendix-datasets-description" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>COCO Caption</strong> <a class="footnote-reference brackets" href="#id7" id="id1">1</a>: The dataset contains over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions are provided.</p></li>
<li><p><strong>Flickr 8k</strong> <a class="footnote-reference brackets" href="#id8" id="id2">2</a>: The dataset consists of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations.</p></li>
<li><p><strong>Flickr 30k</strong> <a class="footnote-reference brackets" href="#id9" id="id3">3</a>: The dataset is an extension of the Flickr 8k Dataset. It consists of 158,915 crowd-sourced captions describing 31,783 images.</p></li>
<li><p><strong>ImageNetV2</strong> <a class="footnote-reference brackets" href="#id10" id="id4">4</a>: ImageNetV2 contains three test sets with 10,000 new images each. Importantly, these test sets were sampled after a decade of progress on the original ImageNet dataset. This makes the new test data independent of existing models and guarantees that the accuracy scores are not affected by adaptive overfitting.</p></li>
<li><p><strong>VOC2007</strong> <a class="footnote-reference brackets" href="#id11" id="id5">5</a>: The training data provided consists of a set of images; each image has an annotation file giving a bounding box and object class label for each object in one of the twenty classes present in the image. Note that multiple objects from multiple classes may be present in the same image.</p></li>
<li><p><strong>VTAB natural group</strong> <a class="footnote-reference brackets" href="#id12" id="id6">6</a>: The natural group represents classical vision problems. These tasks contain natural images captured using standard cameras. The classes may represent generic, fine-grained, or abstract objects.</p>
<ul>
<li><p><strong>Caltech101</strong>: The task consists in classifying pictures of objects (101 classes plus a background clutter class), including animals, airplanes, chairs, or scissors. The image size varies, but it typically ranges from 200-300 pixels per edge.</p></li>
<li><p><strong>CIFAR-100</strong>: The task consists in classifying natural images (100 classes, with 500 training images each). Some examples include apples, bottles, dinosaurs, and bicycles. The image size is 32x32.</p></li>
<li><p><strong>DTD</strong>: The task consists in classifying images of textural patterns (47 classes, with 120 training images each). Some of the textures are banded, bubbly, meshed, lined, or porous. The image size ranges between 300x300 and 640x640 pixels.</p></li>
<li><p><strong>Flowers102</strong>: The task consists in classifying images of flowers present in the UK (102 classes, with between 40 and 248 training images per class). Azalea, Californian Poppy, Sunflower, or Petunia are some examples. Each image dimension has at least 500 pixels.</p></li>
<li><p><strong>Pets</strong>: The task consists in classifying pictures of cat and dog breeds (37 classes with around 200 images each), including Persian cat, Chihuahua dog, English Setter dog, or Bengal cat. Images dimensions are typically 200 pixels or larger.</p></li>
<li><p><strong>Sun397</strong>: The Sun397 task is a scenery benchmark with 397 classes and, at least, 100 images per class. Classes have a hierarchy structure, and include cathedral, staircase, shelter, river, or archipelago. The images are (colour) 200x200 pixels or larger.</p></li>
<li><p><strong>SVHN</strong>: This task consists in classifying images of Googleâ€™s street-view house numbers (10 classes, with more than 1000 training images each). The image size is 32x32 pixels.</p></li>
</ul>
</li>
<li><p><strong>VTAB specialized group</strong>: The specialized group also contains images of the world, but captured through specialist equipment. These images have different invariances to those in the specialized tasks. Nonetheless, humans recognize the structures therein, thus generic visual representations should also capture the visual concepts. It two sub-groups: remote sensing, and medical.</p>
<ul>
<li><p><strong>EuroSAT</strong>: The task consists in classifying Sentinel-2 satellite images into 10 different types of land use (Residential, Industrial, River, Highway, etc). The spatial resolution corresponds to 10 meters per pixel, and the image size is 64x64 pixels.</p></li>
<li><p><strong>Resisc45</strong>: The Remote Sensing Image Scene Classification (RESISC) dataset is a scene classification task from remote sensing images. There are 45 classes, containing 700 images each, including tennis court, ship, island, lake, parking lot, sparse residential, or stadium. The image size is RGB 256x256 pixels.</p></li>
<li><p><strong>Patch Camelyon</strong>: The Patch Camelyon dataset contains 327,680 images of histopathologic scans of lymph node sections. The classification task consists in predicting the presence of metastatic tissue in given image (i.e., two classes). All images are 96x96 pixels.</p></li>
<li><p><strong>Retinopathy</strong>: The Diabetic Retinopathy dataset consists of image-label pairs with high-resolution retina images, and labels that indicate the presence of Diabetic Retinopahy (DR) in a 0-4 scale (No DR, Mild, Moderate, Severe, or Proliferative DR).</p></li>
</ul>
</li>
<li><p><strong>VTAB structured group</strong>: The structured group assesses comprehension of the structure of a scene, for example, object counting, or 3D depth prediction. Most of these tasks are generated from simulated environments, whose structure is easy for a human to determine, but whose domain differs greatly to datasets like ImageNet. These tasks are intended as a step towards useful representations for perceptual control.</p>
<ul>
<li><p><strong>Clevr/count</strong>: CLEVR is a visual question and answer dataset designed to evaluate algorithmic visual reasoning. We use just the images from this dataset, and create a synthetic task by setting the label equal to the number of objects in the images.</p></li>
<li><p><strong>Clevr/distance</strong>: Another synthetic task we create from CLEVR consists of predicting the depth of the closest object in the image from the camera. The depths are bucketed into size bins.</p></li>
<li><p><strong>dSprites/location</strong>: The dSprites dataset was originally designed to asses disentanglement properties of unsupervised learning algorithms. In particular, each image is a 2D shape where six factors are controlled: color, shape, scale, rotation, and (x,y) center coordinates. Images have 64x64 black-and-white pixels. This task consists in predicting the x (horizontal) coordinate of the object. The locations are bucketed into 16 bins.</p></li>
<li><p><strong>dSprites/orientation</strong>: We create another task from dSprites consists in predicting the orientation of each object, bucketed into 16 bins.</p></li>
<li><p><strong>SmallNORB/azimuth</strong>: The Small NORB dataset contains images of 3D-toys from 50 classes, including animals, human figures, airplanes, trucks, and cars. The image size is 640x480 pixels. In this case, we define labels depending on the azimuth (angle of horizontal deviation), in intervals of 20 degrees (18 classes).</p></li>
<li><p><strong>SmallNORB/elevation</strong>: Another synthetic task we create from Small NORB consists in predicting the elevation in the image. There are 9 classes, corresponding to 9 different elevations ranging from 30 to 70 degrees, in intervals of 5 degrees.</p></li>
<li><p><strong>DMLab</strong>: The DMLab (DeepMind Lab) is a set of control environments focused on 3D navigation and puzzle-solving tasks. The Dmlab dataset contains frames observed by the agent acting in the DeepMind Lab environment, which are annotated by the distance between the agent and various objects present in the environment. The goal is to evaluate the ability of a visual model to reason about distances from the visual input in 3D environments. The Dmlab dataset consists of 360x480 color images in 6 classes. The classes are {close, far, very far} x {positive reward, negative reward} respectively.</p></li>
<li><p><strong>KITTI-Dist</strong>: The KITTI task consists in predicting the (binned) depth to the vehicle (car, van, or truck) in the image. There are 4 bins / classes.</p></li>
</ul>
</li>
</ul>
<dl class="footnote brackets">
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://arxiv.org/pdf/1504.00325.pdf">https://arxiv.org/pdf/1504.00325.pdf</a></p>
</dd>
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p><a class="reference external" href="https://www.kaggle.com/datasets/adityajn105/flickr8k">https://www.kaggle.com/datasets/adityajn105/flickr8k</a></p>
</dd>
<dt class="label" id="id9"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p><a class="reference external" href="https://shannon.cs.illinois.edu/DenotationGraph/">https://shannon.cs.illinois.edu/DenotationGraph/</a></p>
</dd>
<dt class="label" id="id10"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/modestyachts/ImageNetV2">https://github.com/modestyachts/ImageNetV2</a></p>
</dd>
<dt class="label" id="id11"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/">http://host.robots.ox.ac.uk/pascal/VOC/voc2007/</a></p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p><a class="reference external" href="https://arxiv.org/pdf/1910.04867.pdf">https://arxiv.org/pdf/1910.04867.pdf</a></p>
</dd>
</dl>
</section>
</section>

                </article>
            </div>
            <footer>
                
                <div class="related-pages">
                    <a class="next-page" href="../retriever/">
                        <div class="page-info">
                            <div class="context">
                                <span>Next</span>
                            </div>
                            <div class="title">CLIP Search</div>
                        </div>
                        <svg class="furo-related-icon">
                            <use href="#svg-arrow-right"></use>
                        </svg>
                    </a>
                    <a class="prev-page" href="../server/">
                        <svg class="furo-related-icon">
                            <use href="#svg-arrow-right"></use>
                        </svg>
                        <div class="page-info">
                            <div class="context">
                                <span>Previous</span>
                            </div>
                            
                            <div class="title">Server API</div>
                            
                        </div>
                    </a>
                </div>
                <div class="bottom-of-page">
                    <div class="left-details">
                        <div class="copyright">
                            Copyright &#169; Jina AI Limited. All rights reserved.
                        </div><div class="last-updated">
                            Last updated on Nov 15, 2022</div>
                    </div>
                    <div class="right-details">
                        <div class="social-btns">
                            <a class='social-btn' href="https://github.com/jina-ai/clip-as-service/" aria-label="GitHub"
                               target="_blank" rel="noreferrer"> <i class="fab fa-github"></i></a>
                            <a class='social-btn' href="https://slack.jina.ai" aria-label="Slack" target="_blank"
                               rel="noreferrer"> <i class="fab fa-slack"></i></a>
                            <a class='social-btn' href="https://youtube.com/c/jina-ai" aria-label="YouTube"
                               target="_blank" rel="noreferrer"> <i class="fab fa-youtube"></i></a>
                            <a class='social-btn' href="https://twitter.com/JinaAI_" aria-label="Twitter"
                               target="_blank" rel="noreferrer"> <i class="fab fa-twitter"></i></a>
                            <a class='social-btn' href="https://www.linkedin.com/company/jinaai/" aria-label="LinkedIn"
                               target="_blank" rel="noreferrer"> <i class="fab fa-linkedin"></i></a>
                        </div>
                    </div>
                </div>
                
            </footer>
        </div>
        <aside class="toc-drawer">
            
            
            <div class="toc-sticky toc-scroll">
                <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
                </div>
                <div class="toc-tree-container">
                    <div class="toc-tree">
                        <ul>
<li><a class="reference internal" href="#">Benchmark</a><ul>
<li><a class="reference internal" href="#select-the-right-model">Select the right model</a></li>
<li><a class="reference internal" href="#size-and-efficiency">Size and efficiency</a></li>
<li><a class="reference internal" href="#zero-shot-performance">Zero-shot performance</a><ul>
<li><a class="reference internal" href="#zero-shot-retrieval">Zero-shot retrieval</a></li>
<li><a class="reference internal" href="#zero-shot-classification">Zero-shot classification</a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-datasets-description">Appendix: Datasets description</a></li>
</ul>
</li>
</ul>

                    </div>
                </div>
            </div>
            
            
        </aside>
    </div>
    <qa-bot
            title="CLIP-as-service"
            description="Embed images and sentences into fixed-length vectors with CLIP"
    >
        <template>
            <dl>
                <dt>You can ask questions about our docs. Try:</dt>
                <dd>What is clip-as-service?</dd>
                <dd>How to install clip-as-service?</dd>
                <dd>Where can I get help?</dd>
            </dl>
        </template>
    </qa-bot>
</div>
<img referrerpolicy="no-referrer-when-downgrade"
     src="https://static.scarf.sh/a.png?x-pxid=2823e771-0e1e-4320-8fde-48bc48e53262"/><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() { 
            document.querySelector('qa-bot').setAttribute('server', 'https://jina-ai-clip-as-service.docsqa.jina.ai');
        });
        </script>
    <script src="https://cdn.jsdelivr.net/npm/vue@2/dist/vue.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/qabot@0.4"></script>
    </body>
</html>